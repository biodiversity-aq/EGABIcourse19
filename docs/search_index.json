[
["index.html", "SCAR-EGABI Tools for Southern Ocean Spatial Analysis and Modelling 1 About this Technical stuff", " SCAR-EGABI Tools for Southern Ocean Spatial Analysis and Modelling Anton Van de Putte, Charlène Guillaumot, Grant Humphries, Huw Griffiths, Ben Raymond, Ryan Reisinger 1 About this This document provides material for the SCAR-EGABI Tools for Southern Ocean Spatial Analysis and Modelling Course to be held in Leuven, Belgium in September 2019. Issues or suggestions about the content can be raised through the issue tracker (GitHub account required). Technical stuff This material has been written using bookdown and R. To build the book locally, clone the repo, open R and run the following lines: setwd(&quot;the/path/to/your/copy/of/the/repo&quot;) bookdown::render_book(&quot;index.Rmd&quot;) And view it with: browseURL(&quot;docs/index.html&quot;) "],
["course-schedule.html", "2 Course schedule 2.1 Monday 2.2 Tuesday 2.3 Wednesday 2.4 Thursday 2.5 Friday", " 2 Course schedule 2.1 Monday Time Item 9:00 Course introduction, housekeeping, presentation of participants 10:00 Break 10:15 Introduction to R and the Tidyverse 11:15 Break 11:30 Open data and data cleaning 12:30 Lunch 13:00 Finding and downloading data through the rOpenSci packages 14:00 Break 14:15 Mapping 15:15 Break 15:30 Data visualisation 17:30 End day 1 2.2 Tuesday Time Item 9:00 What are species distribution models? 10:00 Break 10:15 Recent and past application of SDMs 11:15 Break 11:30 Preparation for exercises 12:30 Lunch 13:30 Run a simple SDM, understand and generate SDM outputs 15:00 Break 15:15 SDM calibration: what you should know before running a model 17:30 End day 2 2.3 Wednesday Time Item 9:30 Introduction to different SDM algorithms 10:30 Break 10:45 Questions, complete previous courses if necessary 12:30 Lunch 13:30 Personal work 17:30 End day 3 2.4 Thursday Time Item 9:30 Personal work (with breaks) 17:30 End Day 4 2.5 Friday Time Item 9:30 Personal work, final presentations, and discussion (with breaks) 17:00 End Day 5 "],
["course-introduction.html", "3 Course Introduction", " 3 Course Introduction Presentation (PDF) "],
["overview.html", "4 Overview 4.1 Preparation 4.2 Taxonomy 4.3 Occurrences 4.4 Environmental data 4.5 Fit model 4.6 Predict from model 4.7 Other bits and pieces", " 4 Overview An example analysis that very briefly demonstrates some of the tasks that we’ll tackle during the workshop. 4.1 Preparation Make sure we have the packages we need from CRAN: pkgs &lt;- c(&quot;dplyr&quot;, &quot;ncdf4&quot;, &quot;raster&quot;, &quot;remotes&quot;, &quot;robis&quot;, &quot;worrms&quot;) pkgs &lt;- setdiff(pkgs, installed.packages()[, 1]) if (length(pkgs) &gt; 0) install.packages(pkgs) library(dplyr) And from GitHub: ## packages with required minimum version pkgs &lt;- c(&quot;SCAR/antanym&quot; = NA, &quot;AustralianAntarcticDivision/blueant&quot; = NA, &quot;AustralianAntarcticDivision/SOmap&quot; = &quot;0.3.0&quot;) for (pkg in names(pkgs)) { if (!basename(pkg) %in% installed.packages()[, 1] || (!is.na(pkgs[[pkg]]) &amp;&amp; packageVersion(basename(pkg)) &lt; pkgs[[pkg]])) { remotes::install_github(pkg) } } 4.2 Taxonomy library(worrms) my_species &lt;- &quot;Euphausia crystallorophias&quot; tax &lt;- wm_records_names(name = my_species) tax ## [[1]] ## # A tibble: 1 x 27 ## AphiaID url ## &lt;int&gt; &lt;chr&gt; ## 1 236216 http://www.marinespecies.org/aphia.php?p=taxdetails&amp;id=236216 ## scientificname authority status ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Euphausia crystallorophias Holt &amp; Tattersall, 1906 accepted ## unacceptreason taxonRankID rank valid_AphiaID ## &lt;lgl&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 NA 220 Species 236216 ## valid_name valid_authority parentNameUsageID ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Euphausia crystallorophias Holt &amp; Tattersall, 1906 110673 ## kingdom phylum class order family genus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Animalia Arthropoda Malacostraca Euphausiacea Euphausiidae Euphausia ## citation ## &lt;chr&gt; ## 1 Siegel, V. (Ed) (2019). World Euphausiacea Database. Euphausia crystallo~ ## lsid isMarine isBrackish ## &lt;chr&gt; &lt;int&gt; &lt;lgl&gt; ## 1 urn:lsid:marinespecies.org:taxname:236216 1 NA ## isFreshwater isTerrestrial isExtinct match_type modified ## &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 NA NA NA exact 2010-03-02T17:11:18.233Z ## the Aphia ID (taxonomic ID) of our species my_aphia_id &lt;- tax[[1]]$valid_AphiaID 4.3 Occurrences Get all data from the Antarctic Euphausiacea occurence data from “German Antarctic Marine Living Resources” (GAMLR) Expeditions data set: library(robis) x &lt;- occurrence(datasetid = &quot;cb16377b-56a8-4d95-802d-4eec02466773&quot;) Plot the complete distribution of samples in black, and Euphausia crystallorophias in green: library(SOmap) SOmap_auto(x$decimalLongitude, x$decimalLatitude, input_lines = FALSE, pcol = &quot;black&quot;, pcex = 0.2) with(x %&gt;% dplyr::filter(aphiaID == my_aphia_id), SOplot(decimalLongitude, decimalLatitude, pch = 19, cex = 0.2, col = &quot;green&quot;)) Or as a polar stereo map: basemap &lt;- SOmap(trim = ceiling(max(x$decimalLatitude))+1, bathy_legend = FALSE) plot(basemap) SOplot(x$decimalLongitude, x$decimalLatitude, pch = 19, cex = 0.2, col = &quot;black&quot;) with(x %&gt;% dplyr::filter(aphiaID == my_aphia_id), SOplot(decimalLongitude, decimalLatitude, pch = 19, cex = 0.2, col = &quot;green&quot;)) We’re going to fit a species (presence/absence) distribution model, so first let’s reorganise our data into presence/absence by sampling site: xfit &lt;- x %&gt;% dplyr::rename(lon = &quot;decimalLongitude&quot;, lat = &quot;decimalLatitude&quot;) %&gt;% group_by(lon, lat) %&gt;% dplyr::summarize(present = any(my_aphia_id %in% aphiaID)) 4.4 Environmental data library(blueant) ## put the data into a temporary directory my_data_directory &lt;- tempdir() ## the data source we want data_source &lt;- sources_sdm(&quot;Southern Ocean marine environmental data&quot;) ## fetch the data status &lt;- bb_get(data_source, local_file_root = my_data_directory, verbose = TRUE) ## ## Mon Sep 02 08:43:35 2019 ## Synchronizing dataset: Southern Ocean marine environmental data ## Source URL https://data.aad.gov.au/eds/4742/download ## -------------------------------------------------------------------------------------------- ## ## this dataset path is: C:\\Users\\ben_ray\\AppData\\Local\\Temp\\Rtmpq0b7Uf/data.aad.gov.au/eds/4742 ## building file list ... done. ## downloading file 1 of 1: https://data.aad.gov.au/eds/4742/download ... done. ## decompressing: C:\\Users\\ben_ray\\AppData\\Local\\Temp\\Rtmpq0b7Uf/data.aad.gov.au/eds/4742/download.zip ... extracting 60 files into C:/Users/ben_ray/AppData/Local/Temp/Rtmpq0b7Uf/data.aad.gov.au/eds/4742 ... done. ## ## Mon Sep 02 08:45:42 2019 dataset synchronization complete: Southern Ocean marine environmental data nc_files &lt;- Filter(function(z) grepl(&quot;\\\\.nc$&quot;, z), status$files[[1]]$file) ## create a raster stack of all layers env_stack &lt;- stack(nc_files) ## the first few layers head(names(env_stack)) ## [1] &quot;chla_ampli_alltime_2005_2012&quot; &quot;chla_max_alltime_2005_2012&quot; ## [3] &quot;chla_mean_alltime_2005_2012&quot; &quot;chla_min_alltime_2005_2012&quot; ## [5] &quot;chla_sd_alltime_2005_2012&quot; &quot;depth&quot; Select just the depth and ice_cover_mean layers and extract their values at our sampling locations: env_stack &lt;- subset(env_stack, c(&quot;depth&quot;, &quot;ice_cover_mean&quot;)) temp &lt;- as.data.frame(raster::extract(env_stack, xfit[, c(&quot;lon&quot;, &quot;lat&quot;)])) xfit &lt;- bind_cols(xfit, temp) head(xfit) ## # A tibble: 6 x 5 ## # Groups: lon [5] ## lon lat present depth ice_cover_mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -122. -73.2 TRUE -660. 0.730 ## 2 -122. -73.2 FALSE -660. 0.730 ## 3 -121. -72.6 FALSE -1409. 0.747 ## 4 -121. -72.6 FALSE -1409. 0.747 ## 5 -121. -72.1 FALSE -1764. 0.726 ## 6 -121. -72.1 FALSE -1764. 0.726 4.5 Fit model We have presence/absence data, so we’ll fit a simple binomial model. The probability of presence of Euphausia crystallorophias is fitted as a smooth function of depth and mean sea ice cover: library(mgcv) fit &lt;- gam(present ~ s(depth) + s(ice_cover_mean), family = binomial, data = xfit) summary(fit) ## ## Family: binomial ## Link function: logit ## ## Formula: ## present ~ s(depth) + s(ice_cover_mean) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.3728 0.3402 -12.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(depth) 3.029 3.792 42.03 2.33e-08 *** ## s(ice_cover_mean) 8.501 8.923 206.88 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.265 Deviance explained = 32.5% ## UBRE = -0.64176 Scale est. = 1 n = 2390 The fits to depth and ice cover: plot(fit, pages = 1, shade = TRUE) This suggests that we are more likely to find Euphausia crystallorophias in shallow areas with high annual ice cover, which seems plausible given that this species is typically found in coastal/shelf waters. 4.6 Predict from model xpred &lt;- expand.grid(lon = seq(from = floor(min(xfit$lon)), to = ceiling(max(xfit$lon)), by = 0.25), lat = seq(from = floor(min(xfit$lat)), to = ceiling(max(xfit$lat)), by = 0.25)) xpred &lt;- bind_cols(as.data.frame(xpred), as.data.frame(raster::extract(env_stack, xpred[, c(&quot;lon&quot;, &quot;lat&quot;)]))) xpred$predicted &lt;- predict(fit, newdata = xpred, type = &quot;response&quot;) ## create raster pr &lt;- rasterFromXYZ(xpred[, c(&quot;lon&quot;, &quot;lat&quot;, &quot;predicted&quot;)]) projection(pr) &lt;- &quot;+proj=longlat +datum=WGS84&quot; my_cmap &lt;- if (getRversion() &gt;= &quot;3.6&quot;) hcl.colors(21, &quot;Geyser&quot;) else c(&quot;#008585&quot;, &quot;#359087&quot;, &quot;#539B8A&quot;, &quot;#6DA590&quot;, &quot;#85AF97&quot;, &quot;#9BBAA0&quot;, &quot;#AEC4AA&quot;, &quot;#BED0B0&quot;, &quot;#D0DCB5&quot;, &quot;#E5E7BC&quot;, &quot;#FBF2C4&quot;, &quot;#F3E3B2&quot;, &quot;#EDD59F&quot;, &quot;#E7C68C&quot;, &quot;#E3B77A&quot;, &quot;#DEA868&quot;, &quot;#DA9857&quot;, &quot;#D58847&quot;, &quot;#D1773A&quot;, &quot;#CC6530&quot;, &quot;#C7522B&quot;) Plot it: p &lt;- SOmap_auto(x = pr, bathy = pr) p$bathy_palette &lt;- my_cmap p plot(basemap) SOplot(pr, col = my_cmap) 4.7 Other bits and pieces 4.7.1 Place names library(antanym) ## The Composite Gazetteer of Antarctica is made available under a CC-BY license. ## If you use it, please cite it: ## Composite Gazetteer of Antarctica, Scientific Committee on Antarctic Research. GCMD Metadata (http://gcmd.nasa.gov/records/SCAR_Gazetteer.html) ## get full SCAR gazetteer data xn &lt;- an_read(cache = &quot;session&quot;, sp = TRUE) ## reduce to one name per feature xn &lt;- an_preferred(xn, origin = &quot;United Kingdom&quot;) ## ask for suggestions in our region to show on our map xns &lt;- an_suggest(xn, map_scale = 20e6, map_extent = extent(pr)) ## transform to our map projection, convert to data frame, take the top 10 xns &lt;- as_tibble(SOproj(xns, target = basemap$projection)) %&gt;% head(10) Add them to the map: plot(basemap) SOplot(pr, col = my_cmap) ## placename points with(xns, points(x = longitude, y = latitude, pch = 16, cex = 0.4)) ## and labels with(xns, text(x = longitude, y = latitude, labels = place_name, cex = 0.75, pos = 2 + 2*(seq_len(nrow(xns)) %% 2))) "],
["introduction-to-r-and-the-tidyverse.html", "5 Introduction to R and the Tidyverse 5.1 Exercise R script 5.2 See also", " 5 Introduction to R and the Tidyverse Presentation (PDF) 5.1 Exercise R script The R script can be downloaded here and is also reproduced below. ## We load Tidyverse here, which includes a suite of packages that can work together. ## When loading directly like below, you get a table which tells you what packages are attached library(tidyverse) ## you will also see a list of conflicted functions - that is, function names that appear ## in multiple packages. For example, there is a &quot;filter&quot; function in both the stats package ## and in the dplyr package. See the course notes about namespace prefixes (and e.g. ## the &quot;dplyr::filter()&quot; usages below) ## Load the starwars dataset found in the dplyr package (we&#39;ll use this later!) data(starwars) ################################################################# #### Data importing #### ######################## setwd(&quot;c:/EGABIcourse19-master/docs/course_material/tidyverse&quot;) ### Using base R read.csv the column classes are imported poorly and we would have to ## convert them ourselves later!! X.csv &lt;- read.csv(&#39;tidyverse_dummy_import.csv&#39;) sapply(X.csv, class) ## the readr library can read in csvs as well, except this is done intelligently and fields ## can be defined better X.csv_r &lt;- readr::read_csv(&#39;tidyverse_dummy_import.csv&#39;, col_types=list(Occurrence_field = col_factor())) ## The readxl library is for reading other excel formats and WORKS WELL FOR DATETIME columns ## However, it does not have a column type option for factors ## (if you wanted to define a column as a factor) X.xlsx_t &lt;- readxl::read_xlsx(&#39;tidyverse_dummy_import.xlsx&#39;,1, col_types = c(&#39;guess&#39;,&#39;numeric&#39;,&#39;numeric&#39;,&#39;text&#39;,&#39;date&#39;)) ## you could make a column a factor, though, if you needed to: X.xlsx_t$Occurrence_field &lt;- as.factor(X.xlsx_t$Occurrence_field) ################################################################### #### Data wrangling #### ######################## # select the name, gender and homeworld columns starwars %&gt;% dplyr::select(name,gender,homeworld) ## note that we use dplyr::select(), not just select(), because there are several &quot;select&quot; ## functions in different packages. Using the namespace prefix makes it clear which one ## we are using and less prone to errors caused by e.g. packages being loaded in a ## different order # r base equivalent starwars[,c(&#39;name&#39;,&#39;gender&#39;,&#39;homeworld&#39;)] # filter rows with characters from Tatooine starwars %&gt;% dplyr::filter(homeworld==&#39;Tatooine&#39;) # r base equivalent starwars[which(starwars$homeworld==&#39;Tatooine&#39;),] # stringing arguments starwars %&gt;% dplyr::select(name,gender,homeworld)%&gt;% dplyr::filter(homeworld==&#39;Tatooine&#39;) #r base equivalent starwars[which(starwars$homeworld==&#39;Tatooine&#39;),c(&#39;name&#39;,&#39;gender&#39;,&#39;homeworld&#39;)] ################################## #### grouping and summarising #### ################################## starwars %&gt;% group_by(homeworld) ## This gets the mean height of characters, grouped by homeworld, and adds a count to ## get sample size starwars %&gt;% group_by(homeworld) %&gt;% summarise(mean_height = mean(height),n = n()) ## This gets the mean height of characters, grouped by homeworld, and adds a count to ## get sample size starwars %&gt;% group_by(homeworld,gender) %&gt;% summarise(mean_height = mean(height),n = n()) %&gt;% dplyr::filter(!is.na(homeworld)) ### Other ways to count! starwars %&gt;% count(homeworld,gender) %&gt;% dplyr::filter(!is.na(homeworld)) starwars %&gt;% add_count(homeworld,gender) %&gt;% dplyr::filter(!is.na(homeworld)) ################ #### TASKS #### ## Create a table with mean median and standard deviation of mass for all humans with brown hair starwars %&gt;% group_by(hair_color,species) %&gt;% summarise(meanmass=mean(mass,na.rm = T), medmass=median(mass,na.rm = T), sdmass=sd(mass,na.rm = T)) %&gt;% dplyr::filter(species == &#39;Human&#39;,hair_color==&#39;brown&#39;) ## Get the tallest and shortest non-human character starwars %&gt;% dplyr::filter(species != &#39;Human&#39;) %&gt;% arrange(height) starwars %&gt;% dplyr::filter(species != &#39;Human&#39;) %&gt;% arrange(-height) ## How many droids were in Attack of the Clones? starwars %&gt;% unnest(films) %&gt;% count(species,films) %&gt;% dplyr::filter(species==&#39;Droid&#39;,films==&#39;Attack of the Clones&#39;) ###################################### #### Mutate #### # This will calculate a new column that is the ration of height to mass starwars %&gt;% mutate(hm_ratio = height/mass) ###################################### #### join #### df1 &lt;- tibble( category = c(&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;c&#39;,&#39;a&#39;,&#39;b&#39;), id = c(1,2,3,4,5,6,7) ) df2 &lt;- tibble( category = c(&#39;b&#39;,&#39;a&#39;,&#39;b&#39;,&#39;a&#39;,&#39;a&#39;,&#39;c&#39;,&#39;c&#39;), id = c(7,6,5,4,3,2,1) ) ## This will join the two tibbles by the id value. df1 %&gt;% left_join(df2,by=&#39;id&#39;) ################################################################# #### Gather and Spread #### ########################### ### This is a dataset that you might come across - different devices taking ## measurements on the same day sst_data &lt;- data.frame( time = as.Date(&#39;2009-01-01&#39;) + 0:9, unit_A = round(runif(10, 6, 9),2), unit_B = round(runif(10, 7, 10),2), unit_C = round(runif(10, 6, 10),2) ) ## Gathering makes the dataframe go from wide to LONG format (Long format ## typically used by ggplot2) sst_data %&gt;% gather(unit,sst,-time) ## spread is the opposite function to gather and goes from LONG to WIDE format sst_data %&gt;% gather(unit,sst,-time) %&gt;% spread(unit,sst) ############################### #### Nesting #### ## This nests the unit and sst columns into a table for every time column. nested_sst &lt;- sst_data %&gt;% gather(unit,sst,-time) %&gt;% nest(unit,sst) ## This can be used to view a specific data row table nested_sst$data[[1]] ############################### #### Tasks 2 #### ################################# ## A tibble with star wars films ranked (by my preference) film.rankings &lt;- tibble( films = c(&#39;The Phantom Menace&#39;,&#39;Attack of the Clones&#39;,&#39;Revenge of the Sith&#39;, &#39;A New Hope&#39;,&#39;The Empire Strikes Back&#39;,&#39;Return of the Jedi&#39;, &#39;The Force Awakens&#39;), rank = c(7,6,5,3,4,1,2) ) ## What films does the character Plo Koon appear in, ordered by rank? The final result ## should be a single table drawn from a list column Plo.Koon &lt;-starwars %&gt;% dplyr::select(name,films) %&gt;% unnest(films) %&gt;% left_join(film.rankings,by=&#39;films&#39;) %&gt;% arrange(name,rank) %&gt;% nest(films,rank) %&gt;% dplyr::filter(name == &#39;Plo Koon&#39;) ## Create a table that nests height and mass data for only humans in a column nested ## by film name. nested.mass.height &lt;- starwars %&gt;% unnest(films)%&gt;% dplyr::select(name,height,mass,films,species) %&gt;% dplyr::filter(species==&#39;Human&#39;)%&gt;% nest(name,height,mass) ################################################################################ #### Final task - functional programming ## Create an object that is a list of vectors X &lt;- c(list(seq(1,50,5)),list(seq(40,100,2))) ## base R lapply(X,mean) ## tidyverse map(X,mean) ### This shows how you can use map to run functions over lists to get interesting model ## results for large arrays of data. Very helpful for summarising data quickly nested.mass.height %&gt;% mutate( model = map( data,~lm(mass~height,data=.x) ), slope = map_dbl( model, ~coef(.x)[&#39;height&#39;] ), r.sq = map_dbl( model,~glance(.x)$&#39;r.squared&#39; ), sample_size = map_dbl( data,~nrow(.x) ), tallest = map( data,~.x[which.max(.x$height),&#39;name&#39;] ), heaviest = map( data,~.x[which.max(.x$mass),&#39;name&#39;] ) ) %&gt;% unnest(tallest,heaviest) %&gt;% rename(tallest=name,heaviest=name1) ######################################################################### #### ggplot2 link #### starwars %&gt;% dplyr::filter(species==&#39;Human&#39;,gender==&#39;male&#39;) %&gt;% ggplot(aes(x=mass,y=height))+ geom_point()+ geom_smooth(method=lm) 5.2 See also The Software Carpentry Introduction to R and RStudio. An Introduction to R by the Ocean Biogeographic Information System (OBIS). Getting Started in R — Tidyverse Edition by Saghir Bashir. "],
["open-data-and-data-cleaning.html", "6 Open data and data cleaning 6.1 FAIR principles", " 6 Open data and data cleaning 6.1 FAIR principles Presentation (PDF) See also the Metadata - Darwin Core section. "],
["metadata-darwin-core.html", "7 Metadata - Darwin Core 7.1 Darwin Core structure 7.2 Types of Darwin Core archives 7.3 Darwin Core details 7.4 Metadata for occurrence data", " 7 Metadata - Darwin Core The Darwin Core (DwC) is a body of standards maintained by TDWG (Biodiversity Information Standards, formerly known as The International Working Group on Taxonomic Databases). It includes a glossary of terms intended to facilitate the sharing of information about biological diversity by providing reference definitions, examples, and commentaries. The Darwin Core is primarily based on taxa, their occurrence in nature as documented by observations, specimens, and samples, and related information. Darwin Core revolves around a standard file format, the Darwin Core Archive (DwC-A). This compact package (a ZIP file) contains interconnected text files and enables data publishers to share their data using a common terminology. DwC is used both by GBIF and OBIS for publishing data. This standardization not only simplifies the process of publishing biodiversity datasets, it also makes it easy for users to discover, search, evaluate and compare datasets as they seek answers to today’s data-intensive research and policy questions. 7.1 Darwin Core structure A DWC-A can contains 4 types of components. These files can include: eml.xml core file extension files (optional) meta.xml EML stands for Ecological Metadata Language. This file contains the dataset metadata in an XML format. Core and extension files contain data records, arranged one per line. Each row in the extension file or extension record points to a single record in the core file. For each single core record there can be many extension records. This is sometimes referred to as a “star schema”. The meta.xml file describes how the files in the archive are organised. It describes the linkage between the core and extension files and maps non-standards column names to Darwin Core terms. To publish the data as a DwC-A, we recommend to upload the core and extension files to the IPT (e.g. IPT.biodiversity.aq). Use the IPT’s built-in metadata editor to enter dataset metadata. The IPT will compile the EML on the data you provided, as well as the meta.xml. 7.2 Types of Darwin Core archives Resource Metadata - Used to describe a biodiversity information resource, including contact details, even if no digital data can currently be shared (this provides a way for researchers to discover resources which are not yet available online). Checklist - Used to share annotated species checklists, taxonomic catalogues, and other information about taxa. Occurrence - Used to share information about a specific instance of a taxon such as a specimen or observation. Event - Used to share information about the protocols used in ecological investigations. 7.3 Darwin Core details In general, metadata EML will accompany some data core in a DwC-A. It provides a description and details on the resource. It is also possible to initially only publish your metadata and add your data at a later stage. This allows researchers to discover resources that or not yet available online. It is best to consider your abstract as the material and methods section of your paper but then for just your data. Try to provide as much detail as possible. Actually if you fill out your metadata extensively you have a first very good draft for a data paper (if that is something you would like to make). You’ll just have to add some maps, summary statistics, and you are good to go. If you are in a hurry, below is an overview of what currently are the required fields. If you use the IPT to create your Darwin Core archive, you will have to choose a short name for your dataset. Choose this wisely because it can’t be changed. The short name serves as an identifier within the IPT installation, and will be used as a parameter in the URL to access the resource via the Internet. For the short name you can only use alphanumeric characters, hyphens, or underscores. 7.3.1 Required metadata fields: 7.3.1.1 Basic Metadata 7.3.1.1.1 title This will be the long title of your dataset, and how the dataset will be cited 7.3.1.1.2 publishing organisation Please select the organisation responsible for publishing (producing, releasing, holding) this resource. It will be used as the resource’s publishing organisation when registering this resource with GBIF and when submitting metadata during DOI registrations. It will also be used to auto-generate the citation for the resource (if auto-generation is turned on), so consider the prominence of the role. Please be aware your selection cannot be changed after the resource has been either registered with GBIF or assigned a DOI. On our IPT you can choose Antarctic Biodiversity Information Facility (AntaBIF) for terrestrial datasets SCAR - AntOBIS for marine datasets SCAR - Microbial Antarctic Resource System for microbial datasets We can also publish on behalf of others. Currently we have agreements with British Antarctic Survey Italian Antarctic National Museum (MNA, section Genua) 7.3.1.1.3 type The type of resource. The value of this field depends on the core mapping of the resource and is no longer editable if the Darwin Core mapping has already been made. This can be Metadata-only Checklist Occurence Event-Core 7.3.1.1.4 metadata language The language in which the metadata document is written. 7.3.1.1.5 data language The primary language in which the described data (not the metadata document) is written. 7.3.1.1.6 Update Frequency The frequency with which changes are made to the resource after the initial resource has been published. For convenience, its value will default to the auto-publishing interval (if auto-publishing has been turned on), however, it can always be overridden later. Please note a description of the maintenance frequency of the resource can also be entered on the Additional Metadata page. 7.3.1.1.7 data license The licence that you apply to a dataset provides a standardized way to define appropriate uses of your work. GBIF encourages publishers to adopt the least restrictive licence possible from among three machine-readable options (CC0 1.0, CC-BY 4.0 or CC-BY-NC 4.0) to encourage the widest possible use and application of data. Learn more here. If you are unable to choose one of the three options, and your dataset contains occurrence data, you will not be able to register your dataset with GBIF or make it globally discoverable through GBIF.org. If you feel unable to select one of the three options, please contact the GBIF Secretariat at participation@gbif.org. 7.3.1.1.8 description A brief overview of the resource that is being documented broken into paragraphs. Think of it as an abstract for a (data) paper 7.3.1.1.9 Resource contact(s) 7.3.1.1.10 Resource creator(s) 7.3.1.1.11 metadata provider(s) Last name Position Organisation 7.3.1.2 Geographic coverage 7.3.1.2.1 description 7.3.2 Citation auto generation Creator 1 R, Creator 2 R, Creator 3 R (2019): How to create a metadata record. v1. Publishing organisation. Dataset/Type. https://ipt.biodiversity.aq/resource?r=shortname&amp;v=1.0 7.3.2.1 Example Griffiths H J, Linse K, Crame J (2017): SOMBASE – Southern Ocean mollusc database: a tool for biogeographic analysis in diversity and evolution. v1.6. British Antarctic Survey. Dataset/Occurrence. https://ipt.biodiversity.aq/resource?r=sombase_southern_ocean_mollusc_database&amp;v=1.6 7.4 Metadata for occurrence data In general, the more information you provide the more knowledge can be gained from your occurrence data. The first user of this data is future yourself. Taking the time to be as detailed in providing and describing your data is time you will gain in the future when you want to reanalyse your data or integrate it in a broader study. The best way is to save the data in some tabular format, something like a csv-file or tab-delimted txt. It is not recomended to use Excel, because it has the habit of giving you unwanted help, resulting in messed up data… Here we start with a description of the minimal metadata that you should provide in order to analyse your data, including developing a species distribution modelling. Each row in your dataset should corresponding to an observation. The names of the columns in the dataset should correspond to DwC terms. You can find a full description of the Darwin Core terms in the Darwin Core quick reference guide. But below we provide an overview of the most important terms. If you have information that doesn’t ft the terms below just check the Darwin Core quick reference guide or create an issue on the EG-ABI course github or the TDWG Github. Your metadata should allow you or anybody else to determine What, Where, When, Who and How. The Darwin Core Standards has 8 absolutely required terms. basisOfRecord, occurrenceID, scientificName, scientificNameID, occurrenceStatus, decimalLongitude, decimalLatitude, eventDate Besides those we recommend a number of other ones as well. In general it is better to provide as many as you can. basisOfRecord type associatedReferences occurrenceID institutionCode collectionCode catalogNumber scientificName, scientificNameID scientificNameAuthorship IdentificationQualifier taxonrank occurrenceStatus organismQuantity organismQuantityType sex lifeStage behavior occurrenceRemarks identifiedBy dateIdentified decimalLongitude, decimalLatitude geodeticDatum coordinateUncertaintyInMeters eventDate year month Day Time Timezone samplingProtocol 7.4.1 What 7.4.1.1 basisOfRecord Additional term type, associatedReferences basisOfRecord describes the nature of record. It has a to follow a specific vocabulary. Some of these might overlap sometimes. That is why it is useful to use type as well. PreservedSpecimen A specimen that has been preserved. This can be a museum specimen, a plant on an herbarium sheet, fish in a bag in a freezer. FossilSpecimen A resource describing a fossilized specimen. LivingSpecimen A resource describing a living specimen. For instance an animal in a zoo. MaterialSample A resource describing a material sample. For instance a material sample from an organism in the wild and sample put into a collection (blood sample, biopsy). HumanObservation A resource describing an observation made by one or more people. For instance an organism observed in the wild and written in field notes. Samples that were collected during an expedition but thrown back into the sea. MachineObservation An output of a machine observation process. Any kind of automated sensor like camera traps. Occurrence A resource describing an instance of the Occurrence class. NOTE: this value is ambiguous and hence should only be used when the resource type is unknown. type The nature or genre of the resource. Must be populated with a value from the DCMI type vocabulary. Options include: Image, StillImage, MovingImage, Interactive Resource, PhysicalObject, Sound, Text, PhysicalObject, Collection, Dataset, Event, Service, Software. associatedReferences A list (concatenated and separated) of identifiers (publication, bibliographic reference, global unique identifier, URI) of literature associated with the occurrence. Example: If you want to add a record from literature you should use basisOfRecord:HumanObservation with type:Text in combination with associatedReferences:Bibliographic reference 7.4.1.2 occurrenceID Additional Terms institutionCode, collectionCode, catalogNumber occurrenceID is an identifier for the occurrence record and should be globally unique and persistent. In the absence of a persistent global unique identifier, construct one from a combination of identifiers in the record that will most closely make the occurrenceID globally unique. Something like the combination of institutionCode, collectionCode, and catalogNumber is a generally used format. There are a number of other terms that you could use as well like datasetName and datasetID. institutionCode The name (or acronym) in use by the institution having custody of the object(s) or information referred to in the record. institutionID An identifier for the institution having custody of the object(s) or information referred to in the record. For physical specimens, the recommended best practice is to use a code and identifier from a collections registry such as the Global Registry of Biodiversity Repositories (http://grbio.org/). Institution code Institution name AAS British Antarctic Survey BRM Alfred-Wegener-Institut für Polar- und Meeresforschung MNA The Museo Nazionale dell’Antartide (Italian National Antarctic Museum in Genoa) RBINS Royal Belgian Institute of Natural Sciences collectionCode The name, acronym, coden, or initialism identifying the collection or data set from which the record was derived. datasetName The name identifying the data set from which the record was derived. datasetID An identifier for the set of data. May be a globally unique identifier or an identifier specific to a collection or institution. catalogNumber An identifier (preferably unique) for the record within the data set or collection. recordNumber An identifier given to the occurrence at the time it was recorded. Often serves as a link between field notes and an occurrence record, such as a specimen collector’s number. occurrenceID institutionCode collectionCode catalogNumber http://arctos.database.museum/guid/MSB:Mamm:233627 MSB Mamm 233627 PS89_FF_000023 PS89 FF 000023 It is a good practice to keep your catalogue/record numbers of equal length throughout your dataset. To keep it persistent just don’t change it. If you reference another source, keep the number. For example if you have samples that were collected during a marine expedition, usually all sampling events get ther own ID. Don’t invent a new ID for the event. This will help you to find any information related to that event at a later stage. 7.4.1.3 scientificName and scientificNameID Additional Terms Kingdom, taxonrank, scientificNameAuthorship, IdentificationQualifier Additional Additional Terms (See Who) identifiedBy, dateIdentified, typeStatus scientificName In the spirit of keeping things persistent this should always contain the originally recorded scientific name, even if the name is currently a synonym (if the sample has been reidentified this is of course a different case). The scientific name should always be to the lowest possible taxonomic rank of which you are certain. In case of uncertain identifications, and the scientific name contains qualifiers such as cf., ? or aff., then this name should go in identificationQualifier. scientificNameAuthorship OBIS recommends to not include authorship in scientificName, and only use scientificNameAuthorship for that purpose. scientificNameID Following the OBIS guidelines a WoRMS LSID should be added in scientificNameID. LSIDs are persistent, location-independent, resource identifiers for uniquely naming biologically significant resources. More information on LSIDs can be found at http://www.lsid.info. taxonrank, Determines the level of identification. This can be for kingdom, phylum, class, order, family, genus, subgenus, specificEpithet, infraspecificEpithet. It is good practice to always provide at least the kingdom. In case you can provide a WoRMS LSID the others are not really required. Otherwise it would be good to add them if possible. identificationQualifier A brief phrase or a standard term (“cf.”, “aff.”) to express the determiner’s doubts about the identification. The use of confer meaning compare and abbeviated to cf. in a scientific name means that the person using the name is saying the animal should be compared to a given species, as it might not be exactly the same species. It’s a way of applying a provisional name to a species and is most frequently used when new fish are discovered that look slightly different to the form normally encountered. It indicates that the fish might be a variant of the same species, but could also turn out to be something completely different. Species affinis (commonly abbreviated to: sp. aff., aff., or affin.) indicates that available material or evidence suggests that the proposed species is related to, has an affinity to, but is not identical to, the species with the binomial name that follows. Species abbreviated as sp. commonly occurs when authors are confident that some individuals belong to a particular genus but are not sure to which exact species they belong. Authors may also use “spp.” (Species pluralis) as a short way of saying that something applies to many species within a genus, but not to all. scientificName scientificNameAuthorship IdentificationQualifier taxonrank 1 Electrona Goode &amp; Bean, 1896 aff. risso genus 2 Electrona antarctica Günther, 1878 species 3 Electrona Goode &amp; Bean, 1896 sp. genus 4 Electrona Goode &amp; Bean, 1896 spp. genus What do these examples mean? Case 1: “I collected something that looks like Electrona risso but I’m not sure.” Case 2: “I’m 100% sure this is Electrona antarctica) Case 3: “I’m sure about the genus, but it could be any one of multiple species” Case 4: “I have this bag with fish; I’m sure about the genus but it could be a variety of species within the genus” 7.4.1.4 occurrenceStatus Additional Terms organismQuantity, organismQuantityType, sex, lifeStage, behavior, occurrenceRemarks occurrenceStatus is a statement about the presence or absence of a taxon at a location. It is an important term, because it allows us to distinguish between presence and absence records. It is an OBIS required term and should be filled in with either “present” or “absent”. organismQuantity, organismQuantityType These two always go together. organismQuantity is a number for the quantity of animals and organismQuantityType defines the type of quantification system used for the quantity of organisms. organismQuantity organismQuantityType 27 individuals 12.5 %biomass r BraunBlanquetScale There are a few other terms as well that we want to mention. The old term that was used was individualCount but that wasn’t really versatile. Still in some cases it can be useful to mention it. Please take note that OBIS recommends all quantitative measurements and sampling facts to be treated in the ExtendedMeasurementOrFact extension and not in the Darwin Core files. OBIS recomends using ExtendedMeasurementOrFact in combination with Darwin Core Event Core, which is a little more advanced. sexdecimalLatitude The sex of the biological individual(s) represented in the occurrence. For the OBIS-recommended vocabulary for sex see BODC vocab : S10 lifeStagedecimalLatitude The age class or life stage of the biological individual(s) at the time the occurrence was recorded. The OBIS recommended vocabulary for lifestage is BODC vocab: S11 behaviordecimalLatitude The behavior shown by the subject at the time the occurrence was recorded (no OBIS recomended vocab available). occurrenceRemarksdecimalLatitude can hold any comments or notes about the occurrence. 7.4.2 Where 7.4.2.1 decimalLatitude anddecimalLongitude Additional Terms geodeticDatum, coordinateUncertaintyInMeters, locality, locationID, footprintWKT To determine the location of a point on a globe you need at least the decimalLatitude and decimalLongitude and and a definition of the spatial reference system that was used. The spatial reference system defines the model of the earths shape that was used. The system currently used by GPS is WGS84 (also known as WGS 1984, EPSG:4326). Quite often the geodeticDatum is not provided but it is actually essential. decimalLatitude defines the position (in degrees) relative to the equator and ranges from 90 (North Pole) to -90 (South Pole). decimalLongitude defines the position (in degrees) relative to the Greenwich Meridian and ranges from 180 to -180. Mak sure not to switch those around. The standard notation has them as latitude followed by longitude. Some people naturally tend to write them as longitude followed by latitude, because this is conceptually similar to a cartesian (x-y) notation. However, especially for samples collected in the area of the Antarctic Peninsula, these switches can be hard to spot. coordinateUncertaintyInMeters The horizontal distance (in meters) from the given decimalLatitude and decimalLongitude describing the smallest circle containing the whole of the location. Leave the value empty if the uncertainty is unknown, cannot be estimated, or is not applicable (because there are no coordinates). Zero is not a valid value for this term. This one is actually closely linked to the number of decimals in your latitude and longitude values. In cases where decimal latitude are calculated they are often expressed with too many digits. If you have more than 6 in biology it doesn’t real make sense anymore or it doesn’t represent how precise the measurement was in the field. To explain it, see this XKCD commic or the table below. decimal places decimal degrees DMS Object that can be unambiguously recognized at this scale N/S or E/W at equator E/W at 67N/S 0 1.0 1° 00’ 0&quot; country or large region 111.32 km 43.496 km 1 0.1 0° 06’ 0&quot; large city or district 11.132 km 4.3496 km 2 0.01 0° 00’ 36&quot; town or village 1.1132 km 434.96 m 3 0.001 0° 00’ 3.6&quot; neighborhood, street 111.32 m 43.496 m 4 0.0001 0° 00’ 0.36&quot; individual street, land parcel 11.132 m 4.3496 m 5 0.00001 0° 00’ 0.036&quot; individual trees, door entrance 1.1132 m 434.96 mm 6 0.000001 0° 00’ 0.0036&quot; individual humans 111.32 mm 43.496 mm 7 0.0000001 0° 00’ 0.00036&quot; practical limit of commercial surveying 11.132 mm 4.3496 mm 8 0.00000001 0° 00’ 0.000036&quot; specialized surveying (e.g. tectonic plate mapping) 1.1132 mm 434.96 µm 7.4.3 When 7.4.3.1 eventDate Additional Terms year, month, Day, Time, Timezone eventDate The date and time when the occurrence/event was recorded. Can be the date-time or interval during which an event occurred. This term uses the ISO 8601 standard. OBIS recommends using the extended ISO 8601 format with hyphens. For time intervals ISO 8601 uses / as a separator. Date and time are separated by T. Times can have a time zone indicator at the end, if this is not the case then the time is assumed to be local time. When a time is UTC, a Z is added. Some examples of ISO 8601 dates are: 1973-02-28T15:25:00 2005-08-31T12:11+12 1993-01-26T04:39+12/1993-01-26T05:48+12 2008-04-25T09:53 1948-09-13 1993-01/02 1993-01 1993 It is an option to split out the eventDate into its seperate components. This can be a good safeguard against software like Excel reformatting your dates and times. year, month, Day, Time, Timezone verbatimEventDate You can put the original date format here, not really needed but might be useful, in case where you are digitising old literature records. 7.4.4 Who identifiedBy, dateIdentified identifiedBy A list (concatenated and separated) of names of people, groups, or organizations who assigned the Taxon to the subject. Recommended best practice is to separate the values in a list with space vertical bar space ( | ) dateIdentified This one is very seldom filled out but it is actually an important one and it should be specific. It should be the person that did the actual identification not the suprvisor. In case of a reindentification it is often useful to be able to contact the person who did the initial identification. 7.4.5 How samplingProtocol This is a tricky one it is quite relevent but this field often doesn’t provide enough space to describe what you did. For describing a specific gear you can use BODC vocab : L22 "],
["biological-data.html", "8 Biological data 8.1 Taxonomy 8.2 Other packages", " 8 Biological data 8.1 Taxonomy The Register of Antarctic Marine Species (RAMS) is the authoritative taxonomic database for Antarctic marine organisms. RAMS is part of the World Register of Marine Species (WoRMS). worrms client for the WoRMS API. Contains mostly taxonomic data, but also trait data. taxize provides access to 20ish sources of taxonomic data sources, including WoRMS. It has consistent data outputs and function interfaces across the different data sources so that you don’t need to tailor your code to different taxonomic data providers. RAMS is currently being extended to cover non-marine taxa, which will become the Register of Antarctic Species (RAS). Hopefully this will remain covered by worrms and the server-side infrastructure hosted by VLIZ. For more detail on R packages dealing with taxonomy in general, see the rOpenSci taxonomy task view. 8.2 Other packages Tracking of animals using satellite, GPS, or light-level geolocation tags is common, and there are many R packages that can help with this. See the spatiotemporal task view for a more complete list. Also of interest may be: TwilightFree provides a method for processing light-level geolocation data that is robust to noise (sensor shading and obscuration) and may be particularly suitable for Southern Ocean applications. traipse provides tools for tracking data, for common metrics of distance, direction, and speed. foieGras fits a continuous-time model (RW or CRW) in state-space form to filter Argos satellite location data. "],
["environmental-data-1.html", "9 Environmental data 9.1 Bowerbird/blueant 9.2 RAADtools 9.3 Other useful packages", " 9 Environmental data 9.1 Bowerbird/blueant Very commonly, we want to know about the environmental conditions at our points of interest. For the remote and vast Southern Ocean these data typically come from satellite or model sources. Some data centres provide extraction tools that will pull out a subset of data to suit your requirements, but often it makes more sense to cache entire data collections locally first and then work with them from there. bowerbird provides a framework for downloading data files to a local collection, and keeping it up to date. The companion blueant package provides a suite of definitions for Southern Ocean and Antarctic data sources that can be used with bowerbird. It encompasses data such as sea ice, bathymetry and land topography, oceanography, and atmospheric reanalysis and weather predictions, from providers such as NASA, NOAA, Copernicus, NSIDC, and Ifremer. Why might you want to maintain local copies of entire data sets, instead of just fetching subsets of data from providers as needed? many analyses make use of data from a variety of providers (in which case there may not be dynamic extraction tools for all of them), analyses might need to crunch through a whole collection of data in order to calculate appropriate statistics (temperature anomalies with respect to a long-term mean, for example), different parts of the same data set are used in different analyses, in which case making one copy of the whole thing may be easier to manage than having different subsets for different projects, a common suite of data are routinely used by a local research community, in which case it makes more sense to keep a local copy for everyone to use, rather than multiple copies being downloaded by different individuals. In these cases, maintaining local copies of a range of data from third-party providers can be extremely beneficial, especially if that collection is hosted with a fast connection to local compute resources (virtual machines or high-performance computational facilities). Install from GitHub: remotes::install_github(&quot;AustralianAntarcticDivision/blueant&quot;) And load the package before use. library(blueant) 9.1.1 Available data sets First, we can see the available data sets via the sources function. srcs &lt;- blueant::sources() ## the names of the first few head(srcs$name) ## [1] &quot;NSIDC SMMR-SSM/I Nasateam sea ice concentration&quot; ## [2] &quot;NSIDC SMMR-SSM/I Nasateam near-real-time sea ice concentration&quot; ## [3] &quot;NSIDC passive microwave supporting files&quot; ## [4] &quot;Nimbus Ice Edge Points from Nimbus Visible Imagery&quot; ## [5] &quot;Artist AMSR-E sea ice concentration&quot; ## [6] &quot;Artist AMSR-E supporting files&quot; ## the full details of the first one srcs[1, ] ## # A tibble: 1 x 16 ## id name ## &lt;chr&gt; &lt;chr&gt; ## 1 10.5067/8GQ8LZQVL0VL NSIDC SMMR-SSM/I Nasateam sea ice concentration ## description ## &lt;chr&gt; ## 1 &quot;Passive microwave estimates of sea ice concentration at 25km spatial re~ ## doc_url source_url ## &lt;chr&gt; &lt;list&gt; ## 1 http://nsidc.org/data/nsidc-0051.html &lt;chr [1]&gt; ## citation ## &lt;chr&gt; ## 1 Cavalieri, D. J., C. L. Parkinson, P. Gloersen, and H. Zwally. 1996, upd~ ## license ## &lt;chr&gt; ## 1 Please cite, see http://nsidc.org/about/use_copyright.html ## comment ## &lt;chr&gt; ## 1 This data source may migrate to https access in the future, requiring an~ ## method postprocess authentication_note user password ## &lt;list&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;named list [5]&gt; &lt;list [0]&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## access_function data_group collection_size ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 raadtools::readice Sea ice 10 9.1.2 Usage Choose a directory into which to download the data. Usually this would be a persistent directory on your machine so that data sets downloaded in one session would remain available for use in later sessions, and not need re-downloading. A persistent directory could be something like c:\\data\\ (on Windows), or you could use the rappdirs package (the user_cache_dir function) to suggest a suitable directory (cross-platform). Here we’ll use the c:/data/cache directory: my_data_dir &lt;- &quot;/data/cache&quot; Select the data source that we want: data_source &lt;- sources(&quot;Southern Ocean marine environmental data&quot;) Note that it’s a good idea to check the dataset size before downloading it, as some are quite large! (Though if you are running the download interactively, it will ask you before downloading a large data set). data_source$collection_size ## size in GB ## [1] 0.1 And fetch the data: result &lt;- bb_get(data_source, local_file_root = my_data_dir, verbose = TRUE) ## ## Mon Sep 02 08:46:36 2019 ## Synchronizing dataset: Southern Ocean marine environmental data ## Source URL https://data.aad.gov.au/eds/4742/download ## -------------------------------------------------------------------------------------------- ## ## this dataset path is: c:\\data\\cache/data.aad.gov.au/eds/4742 ## building file list ... done. ## downloading file 1 of 1: https://data.aad.gov.au/eds/4742/download ... file unchanged on server, not downloading. ## decompressing: c:\\data\\cache/data.aad.gov.au/eds/4742/download.zip ... no new files to extract (not overwriting existing files) ... done. ## ## Mon Sep 02 08:46:38 2019 dataset synchronization complete: Southern Ocean marine environmental data Now we have a local copy of our data. The sync can be run daily so that the local collection is always up to date - it will only download new files, or files that have changed since the last download. For more information on bowerbird, see the package vignette. The result object holds information about the data that we downloaded: result ## # A tibble: 1 x 5 ## name id ## &lt;chr&gt; &lt;chr&gt; ## 1 Southern Ocean marine environmental data 10.26179/5b8f30e30d4f3 ## source_url status files ## &lt;chr&gt; &lt;lgl&gt; &lt;list&gt; ## 1 https://data.aad.gov.au/eds/4742/download TRUE &lt;tibble [61 x 3]&gt; The result$files element tells us about the files: head(result$files[[1]]) ## # A tibble: 6 x 3 ## url ## &lt;chr&gt; ## 1 https://data.aad.gov.au/eds/4742/download ## 2 &lt;NA&gt; ## 3 &lt;NA&gt; ## 4 &lt;NA&gt; ## 5 &lt;NA&gt; ## 6 &lt;NA&gt; ## file ## &lt;chr&gt; ## 1 &quot;c:\\\\data\\\\cache\\\\data.aad.gov.au\\\\eds\\\\4742\\\\download.zip&quot; ## 2 c:/data/cache/data.aad.gov.au/eds/4742/environmental_layers ## 3 c:/data/cache/data.aad.gov.au/eds/4742/environmental_layers/chla_ampli_a~ ## 4 c:/data/cache/data.aad.gov.au/eds/4742/environmental_layers/chla_max_all~ ## 5 c:/data/cache/data.aad.gov.au/eds/4742/environmental_layers/chla_mean_al~ ## 6 c:/data/cache/data.aad.gov.au/eds/4742/environmental_layers/chla_min_all~ ## note ## &lt;chr&gt; ## 1 existing copy ## 2 decompressed ## 3 decompressed ## 4 decompressed ## 5 decompressed ## 6 decompressed These particular files are netCDF, and so could be read using e.g. the raster or ncdf4 packages. However, different data from different providers will be different in terms of grids, resolutions, projections, variable-naming conventions, and other facets, which tends to complicate these operations. In the next section we’ll look at the raadtools package, which provides a set of tools for doing common operations on these types of data. 9.2 RAADtools The raadtools package provides a consistent interface to a range of environmental and similar data, and tools for working with them. It is designed to work data with collections maintained by the bowerbird/blueant packages, and builds on R’s existing ecosystem of packages for working with spatial, raster, and multidimensional data. Here we’ll use two different environmental data sets: sea ice and water depth. Water depth does not change with time but sea ice is provided at daily time resolution. First download daily sea ice data (from 2013 only), and the ETOPO2 bathymetric data set. ETOPO2 is somewhat dated and low resolution compared to more recent data, but will do as a small dataset for demo purposes. This may take a few minutes, depending on your connection speed: src &lt;- bind_rows( sources(&quot;NSIDC SMMR-SSM/I Nasateam sea ice concentration&quot;, hemisphere = &quot;south&quot;, time_resolutions = &quot;day&quot;, years = 2013), sources(&quot;ETOPO2 bathymetry&quot;)) result &lt;- bb_get(src, local_file_root = my_data_dir, clobber = 0, verbose = TRUE, confirm = NULL) ## ## Mon Sep 02 08:46:38 2019 ## Synchronizing dataset: NSIDC SMMR-SSM/I Nasateam sea ice concentration ## ## [... output truncated] Now load the raadtools package and tell it where our data collection has been stored: library(raadtools) set_data_roots(my_data_dir) Let’s say that we have some points of interest in the Southern Ocean — perhaps a ship track, or some stations where we took marine samples, or as we’ll use here, the track of an elephant seal as it moves from the Kerguelen Islands to Antarctica and back again (Data from IMOS 2018[^1], provided as part of the SOmap package). data(&quot;SOmap_data&quot;, package = &quot;SOmap&quot;) ele &lt;- SOmap_data$mirounga_leonina %&gt;% dplyr::filter(id == &quot;ct96-05-13&quot;) Define our spatial region of interest and extract the bathymetry data from this region, using the ETOPO2 files we just downloaded: roi &lt;- round(c(range(ele$lon), range(ele$lat)) + c(-2, 2, -2, 2)) bx &lt;- readtopo(&quot;etopo2&quot;, xylim = roi) And now we can make a simple plot of our our track superimposed on the bathymetry: plot(bx) lines(ele$lon, ele$lat) The real power of raadtools comes from its extraction functions. We can extract the depth values along our track using the raadtools::extract() function. We pass it the data-reader function to use (readtopo), the data to apply it to (ele[, c(&quot;lon&quot;, &quot;lat&quot;)]), and any other options to pass to the reader function (in this case, specifying the topographic data source topo = &quot;etopo2&quot;): ele$depth &lt;- raadtools::extract(readtopo, ele[, c(&quot;lon&quot;, &quot;lat&quot;)], topo = &quot;etopo2&quot;) Plot the histogram of depth values, showing that most of the track points are located in relatively shallow waters: with(ele, hist(depth, breaks = 20)) This type of extraction will also work with time-varying data — for example, we can extract the sea-ice conditions along our track, based on each track point’s location and time: ele$ice &lt;- raadtools::extract(readice, ele[, c(&quot;lon&quot;, &quot;lat&quot;, &quot;date&quot;)]) ## points outside the ice grid will have missing ice values, so fill them with zeros ele$ice[is.na(ele$ice)] &lt;- 0 with(ele, plot(date, ice, type = &quot;l&quot;)) 9.3 Other useful packages the PolarWatch project aims to enable data discovery and broader use of high-latitude ocean remote sensing data sets. The dedicated ERDDAP server (https://polarwatch.noaa.gov/erddap) is accessible to R users with rerddap. rsoi downloads the most up to date Southern Oscillation Index, Oceanic Nino Index, and North Pacific Gyre Oscillation data. satellite reflectance data are a common basis for estimating chlorophyll-a and other phytoplankton parameters at ocean-basin scales. Global products are widely available; however, Southern-Ocean specific algorithms are likely to provide better estimates in these regions. croc implements the Johnson et al. (2013) Southern Ocean algorithm. more broadly, oce provides a wide range of tools for reading, processing, and displaying oceanographic data, including measurements from Argo floats and CTD casts, sectional data, sea-level time series, and coastline and topographic data. fda.oce provides functional data analysis of oceanographic profiles for front detection, water mass identification, unsupervised or supervised classification, model comparison, data calibration, and more. distancetocoast provides “distance to coastline” data for longitude and latitude coordinates. geodist for very fast calculation of geodesic distances. "],
["mapping.html", "10 Mapping 10.1 Maps in R 10.2 SOmap 10.3 Supporting data for maps", " 10 Mapping 10.1 Maps in R The oldest and most core general mapping package in R is the maps package. It has a simple whole-world coastline data set for immediate use. maps::map() The data underlying this live map is available by capturing the output as an actual object. Notice that the coastline for Antarctica does not extend to the south pole, and that parts of Russia that are east of 180 longitude are not in the western part of the map. m &lt;- maps::map(plot = FALSE) lonlat &lt;- cbind(m$x, m$y) plot(lonlat, pch = &quot;+&quot;, cex = 0.4, axes = FALSE) lines(lonlat, col = &quot;dodgerblue&quot;) abline(h = c(-90, 90), v = c(-180, 180)) A very similar and more modern data set is available in the maptools package. data(&quot;wrld_simpl&quot;, package = &quot;maptools&quot;) library(sp) plot(wrld_simpl) This data set aligns exactly to the conventional -180/180 -90/90 extent of the longitude/latitude projection. plot(0, type = &quot;n&quot;, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, xlim = c(-180, 180), ylim = c(-90, 90)) rect(xleft = -180, ybottom = -90, xright = 180, ytop = 90, border = &quot;darkred&quot;, lwd = 4, lty = 2) plot(wrld_simpl, add = TRUE) 10.1.1 Exercises How can we find the longitude and latitude ranges of the maps data m and the maptools data wrld_simpl? Can we draw polygons with a fill colour with the maps package? Answer 1: range(m$x, na.rm = TRUE) range(m$y, na.rm = TRUE) also m$range Answer 2: polygon(lonlat, col = &quot;grey&quot;) does not work, and map(mp, fill = TRUE, col = &quot;grey&quot;) does not work, but maps::map(fill = TRUE, col = &quot;grey&quot;) does seem to work. What’s going on? Look at the very south-eastern corner of the map. The “coastline” has been extended to the very south boundary of the available area. plot(0, type = &quot;n&quot;, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, xlim = c(-150, 180), ylim = c(-90, -60)) plot(wrld_simpl, add = TRUE, col = &quot;grey&quot;) rect(xleft = -180, ybottom = -90, xright = 180, ytop = 90, border = &quot;darkred&quot;, lwd = 4, lty = 2) maps::map(add = TRUE, col = &quot;dodgerblue&quot;, lwd = 3) When we add the old maps coastline see that it does not extend to 90S and it does not traverse the southern boundary. One reason for this is that if we choose a projection where the east and west edges of the Antarctic coastline meet then we get what looks a fairly clean join. ## scale factor f &lt;- 3e6 plot(rgdal::project(lonlat, &quot;+proj=laea +lat_0=-90 +datum=WGS84&quot;), asp = 1, type = &quot;l&quot;, xlim = c(-1, 1) * f, ylim = c(-1, 1) * f, xlab = &quot;&quot;, ylab = &quot;&quot;) If we try the same with wrld_simpl it’s not as neat. We have a strange “seam” that points exactly to the south pole (our projection is centred on longitude = 0, and latitude = -90. plot(sp::spTransform(wrld_simpl, &quot;+proj=laea +lat_0=-90 +datum=WGS84&quot;), asp = 1, xlim = c(-1, 1) * f, ylim = c(-1, 1) * f, xlab = &quot;&quot;, ylab = &quot;&quot;, lwd = 3) abline(v = 0, h = 0, lty = 2, col = &quot;grey&quot;) 10.1.2 Let’s use the maps data! In m we have the maps data structure, and this looks promising. str(m) ## List of 4 ## $ x : num [1:82403] -69.9 -69.9 -69.9 -70 -70.1 ... ## $ y : num [1:82403] 12.5 12.4 12.4 12.5 12.5 ... ## $ range: num [1:4] -180 190.3 -85.2 83.6 ## $ names: chr [1:1627] &quot;Aruba&quot; &quot;Afghanistan&quot; &quot;Angola&quot; &quot;Angola:Cabinda&quot; ... ## - attr(*, &quot;class&quot;)= chr &quot;map&quot; mp &lt;- m pxy &lt;- rgdal::project(lonlat, &quot;+proj=laea +lat_0=-90 +datum=WGS84&quot;) mp$x &lt;- pxy[,1] mp$y &lt;- pxy[,2] mp$range &lt;- c(range(mp$x,na.rm = TRUE), range(mp$y, na.rm = TRUE)) mp$range ## [1] -12709814 12704237 -12576156 12470787 plot(c(-1, 1) * f, c(-1, 1) * f, type = &quot;n&quot;, asp = 1) maps::map(mp, add = TRUE) ## but it doesn&#39;t take much to go awry plot(c(-1, 1) * f, c(-1, 1) * f, type = &quot;n&quot;, asp = 1) maps::map(mp, add = TRUE, fill = TRUE, col = &quot;grey&quot;) The problem is that the maps database has enough internal structure to join lines correctly, with NA gaps between different connected linestrings, but not enough to draw these things as polygons. A similar problem occurs in the default projection. While wrld_simpl has been extend by placing two dummy coordinates at the east and west versions of the south pole, this data set does not have those. We have to look quite carefully to understand what is happening, but this is wrapping around overlapping itself and so close to the southern bound we barely notice. plot(0, type = &quot;n&quot;, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, xlim = c(-180, -110), ylim = c(-90, -60)) rect(xleft = -180, ybottom = -90, xright = 180, ytop = 90, border = &quot;darkred&quot;, lwd = 4, lty = 2) maps::map(add = TRUE,col = &quot;grey&quot;, fill = TRUE) maps::map(col = &quot;grey&quot;, fill = TRUE) mpmerc &lt;- m pxy &lt;- rgdal::project(lonlat, &quot;+proj=merc +datum=WGS84&quot;) mpmerc$x &lt;- pxy[,1] mpmerc$y &lt;- pxy[,2] mpmerc$range &lt;- c(range(mpmerc$x,na.rm = TRUE), range(mpmerc$y, na.rm = TRUE)) mpmerc$range ## [1] -20037508 20037508 -20179524 18351859 ## the catastrophe made a little clearer plot(0, xlim = range(mpmerc$range[1:2]), ylim = c(mpmerc$range[1], 0)) maps::map(mpmerc, fill = TRUE, col = &quot;grey&quot;, add = TRUE) 10.2 SOmap The SOmap package is intended to solve some of these problems, and provide an easier way to produce nice-looking maps of Antarctica and the Southern Ocean. It is primarily focused on maps in polar stereographic projection (although the SOmap_auto function extends this to other projections). SOmap won’t necessarily get you exactly the map you want in every circumstance, but the idea is that in most cases it should get you close enough, and if need be you can make modifications to suit your exact purposes. Please bear in mind that SOmap is still in development, and so its functionality (function parameters and/or behaviour) may change. By default, SOmap works with base graphics (and associated functionality from packages such as raster and sp). It is also possible to work with ggplot2-based graphics, as described below. Start by installing the SOmap package if you haven’t done so already: remotes::install_github(&quot;AustralianAntarcticDivision/SOmap&quot;) Temporary note: the code here was written using the development version of SOmap. You may need to install the development version using remotes::install_github(&quot;AustralianAntarcticDivision/SOmap&quot;, ref = &quot;dev-0.5&quot;). Then load the package: library(SOmap) ## also define a colour map to use for some examples my_cmap &lt;- colorRampPalette(c(&quot;#4D4140&quot;, &quot;#596F7E&quot;, &quot;#168B98&quot;, &quot;#ED5B67&quot;, &quot;#E27766&quot;, &quot;#DAAD50&quot;, &quot;#EAC3A6&quot;))(51) 10.2.1 Circumpolar maps A basic circumpolar map in polar stereographic projection: {r somap1 } SOmap() SOmanagement() provides a number of contextual layers such as MPA boundaries and management zones. SOmap(trim = -40) ## plot to 40S ## add the exclusive economic zones management layer SOmanagement(eez = TRUE) 10.2.1.1 Adding points ## some longitude/latitude data library(sp) my_points_ll &lt;- data.frame(lon = seq(0, 350, by = 10), lat = -55, z = runif(36)) coordinates(my_points_ll) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) projection(my_points_ll) &lt;- &quot;+proj=longlat +datum=WGS84&quot; Our data need to be reprojected to match our map before plotting. The SOproj function does this: ## reproject to our SOmap projection my_points &lt;- SOproj(my_points_ll) ## and plot SOmap() plot(my_points, col = &quot;blue&quot;, add = TRUE) Or use SOplot to reproject and plot in one step: SOmap() SOplot(my_points_ll, col = &quot;blue&quot;) 10.2.1.2 Adding raster layers First let’s construct some artificial raster data (in longitude-latitude space) for demonstration purposes: library(raster) temp &lt;- as.data.frame(expand.grid(lon = seq(100, 140, by = 0.25), lat = seq(-65, -45, by = 0.1))) temp$val &lt;- sqrt((temp$lon - 120)^2/3 + (temp$lat - -40)^2/5) ## create raster object xr &lt;- rasterFromXYZ(temp) projection(xr) &lt;- &quot;+proj=longlat +datum=WGS84&quot; SOplot will reproject and plot this for us: SOmap() SOplot(xr) The legend is out of character with the rest of the map. We can use SOleg to fix that: ## draw the base map SOmap() ## add our raster SOplot(xr, legend = FALSE, col = my_cmap) ## add the legend SOleg(xr, position = &quot;topright&quot;, col = my_cmap, ticks = 6, type = &quot;continuous&quot;, label = &quot;My variable&quot;) OK, well that worked but clearly the labels need tidying up. The easiest way is probably to set the number of decimal places in the label values via the rnd parameter: SOmap() SOplot(xr, legend = FALSE, col = my_cmap) SOleg(xr, position = &quot;topright&quot;, col = my_cmap, ticks = 6, rnd = 2, type = &quot;continuous&quot;, label = &quot;My variable&quot;) Alternatively, we could explicitly set the colour range and labels. ## draw the base map SOmap() ## add our raster, controlling the colour range to span the values 0 to 30 colour_breaks &lt;- seq(0, 30, length.out = length(my_cmap) + 1) SOplot(xr, legend = FALSE, col = my_cmap, breaks = colour_breaks) ## add the legend, again controlling the colour range label_breaks &lt;- seq(0, 30, length.out = 7) SOleg(position = &quot;topright&quot;, col = my_cmap, breaks = label_breaks, type = &quot;continuous&quot;, label = &quot;My variable&quot;) Note that if we don’t want to show the bathymetric legend, we may run into problems: SOmap(bathy_legend = FALSE) ## suppress the bathy legend SOleg(position = &quot;topright&quot;, col = my_cmap, breaks = label_breaks, type = &quot;continuous&quot;, label = &quot;My variable&quot;) The legend has been chopped off because the layout has not left enough space around the map for the curved legend. Currently, the best solution is probably to generate the SOmap object with the bathymetric legend, but then remove it before plotting (see the Modifying map objects section for more details on this): temp &lt;- SOmap() temp$bathy_legend &lt;- NULL ## remove the bathy legend plot(temp) SOleg(position = &quot;topright&quot;, col = my_cmap, breaks = label_breaks, type = &quot;continuous&quot;, label = &quot;My variable&quot;) Multiple rasters: xr2 &lt;- raster::shift(xr, -70) ## offset in longitude SOmap() SOplot(xr, legend = FALSE, col = my_cmap) SOplot(xr2, legend = FALSE, col = my_cmap) 10.2.2 Non-circumpolar maps The SOmap_auto function will take your input data and make a guess at an appropriate projection and extent to use. Note that this is not always going to guess the best projection and extent, so you should view it as a starting point from which you can generate a map to your exact requirements. Use the elephant seal track data bundled with the package: ellie &lt;- SOmap_data$mirounga_leonina ## construct and plot the map SOmap_auto(ellie$lon, ellie$lat) Just a blank map to which you could add other things: SOmap_auto(ellie$lon, ellie$lat, input_points = FALSE, input_lines = FALSE) You can pass a raster as input data, but note that it won’t plot the raster (it uses its extent to infer an appropriate extent for the map): SOmap_auto(xr) But we can add the raster if we wish: SOmap_auto(xr) SOplot(xr, col = my_cmap) We can force a particular projection: SOmap_auto(xr, target = &quot;laea&quot;, centre_lon = 147, centre_lat = -42) SOplot(xr, col = my_cmap) Same but by supplying a full proj4 string to target: SOmap_auto(xr, target = &quot;+proj=laea +lat_0=-42 +lon_0=147&quot;) SOplot(xr, col = my_cmap) See the SOmap_auto vignette for more examples. 10.2.3 Plotting via ggplot2 The SOmap and SOmap_auto functions do their plotting using base graphics. If you are more comfortable working with ggplot2, this is also possible. The SOgg function takes an object created by one of those functions (using base graphics) and converts it to use ggplot2 graphics instead. As with other SOmap functions, this returns an object (of class SOmap_gg or SOmap_auto_gg) that contains all of the information needed to generate the map. Printing or plotting this object will cause it to construct a ggplot object. Printing or plotting that object will cause it to be drawn to the graphics device, just like any other ggplot object. myplot &lt;- SOmap() myplotgg &lt;- SOgg(myplot) ## creates a SOmap_gg object class(myplotgg) ## [1] &quot;SOmap_gg&quot; my_ggplot &lt;- plot(myplotgg) ## creates a ggplot object class(my_ggplot) ## [1] &quot;gg&quot; &quot;ggplot&quot; plot(my_ggplot) ## plot it Or in one step (this will cause myplot to be converted to SOmap’s internal gg format, then a ggplot object constructed from that, then that object will be plotted): SOgg(myplot) 10.2.4 Modifying map objects (advanced usage) The goal of SOmap is to make it fairly easy to produce a fairly-good-looking map that will be adequate for most mapping requirements. It will never be possible to automatically produce a perfect map in every circumstance, but the aim is to have a low-effort way of getting fairly close most of the time. This section describes some approaches to modifying a map to get it closer to your particular needs. Be warned: depending on the exact modifications needed, this might get you pretty close to the crumbling edge of SOmap development. In particular, anything that requires modifying the internal structure of an SOmap object may change in the future (with luck, we’ll make this sort of thing easier - but we’re not there yet.) 10.2.4.1 Modifying base graphics maps Calls to SOmap(), SOmanagement(), SOmap_auto() return an object of class SOmap, SOmap_management, or SOmap_auto. These objects contain all of the data and plotting instructions required to draw the map. Calling print() or plot() on one of these objects will cause that code to be executed, and the object to be drawn in the current graphics device. Hence, calling SOmap() directly without assigning the result to a variable will make it appear in the graphics device, because the returned object is being printed to the console (and thus triggering the print method). But you can also assign the result to a variable, e.g. myplot &lt;- SOmap() and then explicitly plot the object with plot(myplot). The advantage of this is that you can potentially manipulate the myplot object to make changes to the map before plotting it. Note, this is likely to be fragile. Proceed at your own risk! mymap &lt;- SOmap() names(mymap) ## [1] &quot;projection&quot; &quot;target&quot; &quot;straight&quot; &quot;trim&quot; ## [5] &quot;bathy&quot; &quot;box&quot; &quot;plot_sequence&quot; &quot;coastline&quot; ## [9] &quot;ice&quot; &quot;outer_mask&quot; &quot;bathy_legend&quot; &quot;border&quot; The object contains a plot_sequence component, which defines the order in which each part of the plot is drawn. The other components of the object contain the code required to draw each part. Take e.g. the ice component (this is the ice shelves, glacier tongues, etc). This is a list (in this case with only one element). Each element of the list specifies a function to run along with arguments to pass to it: str(mymap$ice) ## List of 1 ## $ :List of 2 ## ..$ plotfun : chr &quot;plot&quot; ## ..$ plotargs:List of 4 ## .. ..$ x :sfc_POLYGON of length 354; first list element: List of 1 ## .. .. ..$ : num [1:5, 1:2] 1022981 1026000 1021994 1021935 1022981 ... ## .. .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;XY&quot; &quot;POLYGON&quot; &quot;sfg&quot; ## .. ..$ col : logi NA ## .. ..$ border: chr &quot;black&quot; ## .. ..$ add : logi TRUE ## ..- attr(*, &quot;class&quot;)= chr &quot;SO_plotter&quot; We can modify the function and/or its arguments: mymap$ice[[1]]$plotargs$col &lt;- &quot;green&quot; plot(mymap) We can remove entire components: temp &lt;- mymap temp$coastline &lt;- NULL temp$ice &lt;- NULL plot(temp) But note that some elements are required. In particular, the bathymetry layer can’t currently be removed because the code that draws this is also the code that creates the plot page via plot.new(). The code below would fail outright if there was no existing existing plot. If there was an existing plot in the graphics device, this code would run but give unpredictable results because it would draw on top of the previously-setup plot: ## code not run here temp &lt;- mymap temp$bathy &lt;- NULL plot(temp) One way around this would be to simply replace all of the bathymetric data values with NAs. The plotting code would still have the extent of the bathymetric layer that it needs in order to set up the plot, but no data would be shown: temp &lt;- mymap ## the bathy data is held in temp$bathy[[1]]$plotargs$x ## and it&#39;s a raster, so we can set its values to NA with raster::values(temp$bathy[[1]]$plotargs$x) &lt;- NA_real_ temp$bathy_legend &lt;- NULL plot(temp) We could also replace the bathymetry data with another raster object. Note that we do need to be careful about the extent and projection of this raster. For example, replacing the bathymetry raster with the ice raster (which has the same polar stereographic projection but smaller extent) gives: temp &lt;- mymap temp$bathy[[1]]$plotargs$x &lt;- ice temp$bathy_legend &lt;- NULL plot(temp) It’s chopped off because the extent of the ice raster is being used to set the plot extent. But if we extend the ice raster to match the map extent: temp &lt;- mymap temp$bathy[[1]]$plotargs$x &lt;- raster::extend(ice, mymap$target) temp$bathy_legend &lt;- NULL plot(temp) 10.2.4.2 Modifying ggplot maps We can modify ggplot2-based maps at two levels. 10.2.4.2.1 Modifying the ggplot object. Remember that printing or plotting a SOmap_gg object produces a ggplot object. This can be modified by adding e.g. layers or themes just like a normal ggplot. Remember to load the ggplot2 library now that we are using ggplot2 functions directly. library(ggplot2) my_ggplot + geom_point(data = as.data.frame(my_points), aes(lon, lat, colour = z), size = 3) + scale_colour_distiller(palette = &quot;Spectral&quot;) Multiple rasters or multiple sets of points gets tricky if they are on different scales, because ggplot2 is only designed to work with a single colour scale per geometry type. You can try your luck with the ggnewscale or relayer packages, although both are in a fairly experimental stage of development. ## remotes::install_github(&quot;clauswilke/relayer&quot;) library(relayer) plot(SOgg(SOmap(straight = TRUE))) + rename_geom_aes(geom_raster(data = as.data.frame(SOproj(xr), xy = TRUE), aes(x = x, y = y, fill2 = val)), new_aes = c(fill = &quot;fill2&quot;)) + scale_fill_gradientn(aesthetics = &quot;fill2&quot;, colors = my_cmap, na.value = NA, name = &quot;My variable&quot;, guide = &quot;legend&quot;) ## remotes::install_github(&quot;eliocamp/ggnewscale&quot;) library(ggnewscale) plot(SOgg(SOmap(straight = TRUE))) + new_scale_fill() + geom_raster(data = as.data.frame(SOproj(xr), xy = TRUE), aes(x = x, y = y, fill = val)) + scale_fill_gradientn(colors = my_cmap, na.value = NA, name = &quot;My variable&quot;) 10.2.4.2.2 Modifying the SOmap_gg object SOmap_gg objects are similar in structure to SOmap objects, in that they contain all of the data and plotting instructions required to draw the map: names(myplotgg) ## [1] &quot;projection&quot; &quot;target&quot; &quot;straight&quot; &quot;trim&quot; ## [5] &quot;init&quot; &quot;bathy&quot; &quot;coord&quot; &quot;plot_sequence&quot; ## [9] &quot;scale_fill&quot; &quot;bathy_legend&quot; &quot;coastline&quot; &quot;ice&quot; ## [13] &quot;axis_labels&quot; &quot;theme&quot; &quot;border&quot; However, instead of base plotting functions, SOmap_gg objects use ggplot2 function calls, e.g.: myplotgg$ice[[1]]$plotfun ## [1] &quot;ggplot2::geom_sf&quot; We can modify these functions and/or arguments in a similar manner to SOmap objects. myplotgg$ice[[1]]$plotargs$fill &lt;- &quot;green&quot; plot(myplotgg) Or remove the bathymetric raster layer: temp &lt;- myplotgg temp$bathy &lt;- NULL temp$bathy_legend &lt;- NULL plot(temp) Or replace it with a different raster (use the ice raster as an example): temp &lt;- myplotgg ## convert ice raster to suitable data.frame ice_raster_as_df &lt;- raster::as.data.frame(SOproj(ice), xy = TRUE) names(ice_raster_as_df)[3] &lt;- &quot;ice&quot; ## add this to our object in place of bathy temp$bathy &lt;- SO_plotter(plotfun = &quot;ggplot2::geom_raster&quot;, plotargs = list(data = ice_raster_as_df, mapping = aes_string(fill = &quot;ice&quot;)) ) ## change the colour scale temp$scale_fill[[1]]$plotargs &lt;- list(colours = my_cmap, na.value = &quot;#FFFFFF00&quot;, guide = FALSE) ## remove the bathy legend temp$bathy_legend &lt;- NULL plot(temp) 10.3 Supporting data for maps When constructing maps, we commonly want to show features like oceanographic fronts, ice extent, coastline, place names, and MPA boundaries. There are a few sources of such data: some layers are bundled into SOmap, see the SOmap::SOmap_data object antanym provides access to the SCAR Composite Gazetteer of place names the quantarcticR package provides access to Quantarctica data layers. 10.3.1 quantarcticR Note, this package is still in development, so the usage as shown here might change in later versions. Install if needed: remotes::install_github(&quot;SCAR-sandpit/quantarcticR&quot;) Example usage: library(quantarcticR) ## Quantarctica is made available under a CC-BY license. If you use it, please cite it: ## Matsuoka K, Skoglund A, Roth G (2018) Quantarctica [Data set]. Norwegian Polar Institute. https://doi.org/10.21334/npolar.2018.8516e961 ## In addition, published works produced using Quantarctica are asked to cite each dataset that was used in the work. Please consult the abstract of each data set for the relevant citation. ## ## QuantarcticR is using a temporary data directory for this session: see the `qa_cache_dir` function to change this. ds &lt;- qa_datasets() ## all available layers head(ds) ## # A tibble: 6 x 5 ## layername ## &lt;chr&gt; ## 1 Overview place names ## 2 COMNAP listed facilities ## 3 Subantarctic stations ## 4 SCAR Composite gazetteer ## 5 IBO-IOC GEBCO Features (point) ## 6 IBO-IOC GEBCO Features (multipoint) ## main_file ## &lt;chr&gt; ## 1 &quot;C:\\\\Users\\\\ben_ray\\\\AppData\\\\Local\\\\Temp\\\\Rtmpq0b7Uf/quantarcticR-cache~ ## 2 &quot;C:\\\\Users\\\\ben_ray\\\\AppData\\\\Local\\\\Temp\\\\Rtmpq0b7Uf/quantarcticR-cache~ ## 3 &quot;C:\\\\Users\\\\ben_ray\\\\AppData\\\\Local\\\\Temp\\\\Rtmpq0b7Uf/quantarcticR-cache~ ## 4 &quot;C:\\\\Users\\\\ben_ray\\\\AppData\\\\Local\\\\Temp\\\\Rtmpq0b7Uf/quantarcticR-cache~ ## 5 &quot;C:\\\\Users\\\\ben_ray\\\\AppData\\\\Local\\\\Temp\\\\Rtmpq0b7Uf/quantarcticR-cache~ ## 6 &quot;C:\\\\Users\\\\ben_ray\\\\AppData\\\\Local\\\\Temp\\\\Rtmpq0b7Uf/quantarcticR-cache~ ## type cached download_size ## &lt;chr&gt; &lt;lgl&gt; &lt;fs::bytes&gt; ## 1 shapefile FALSE 19.74K ## 2 shapefile FALSE 691.92K ## 3 shapefile FALSE 691.92K ## 4 shapefile FALSE 329.05M ## 5 shapefile FALSE 1.25M ## 6 shapefile FALSE 1.25M ## more info about a particular layer my_layer &lt;- qa_dataset(&quot;Median sea ice extent 1981-2010&quot;) my_layer ## # A tibble: 1 x 11 ## layername ## &lt;chr&gt; ## 1 Median sea ice extent 1981-2010 ## datasource ## &lt;chr&gt; ## 1 SeaIce/Median Ice Extents/Median_SeaIce_Extents_1981-2010.shp ## layer_attributes srs_attributes provider ## &lt;list&gt; &lt;list&gt; &lt;chr&gt; ## 1 &lt;NULL&gt; &lt;tibble [1 x 4]&gt; ogr ## abstract ## &lt;chr&gt; ## 1 &quot;Monthly median sea ice extents for the period 1981-2010.\\n\\nFetterer, F~ ## extent palette type download_size ## &lt;list&gt; &lt;list&gt; &lt;chr&gt; &lt;fs::bytes&gt; ## 1 &lt;NULL&gt; &lt;NULL&gt; shapefile 131K ## main_file ## &lt;chr&gt; ## 1 &quot;C:\\\\Users\\\\ben_ray\\\\AppData\\\\Local\\\\Temp\\\\Rtmpq0b7Uf/quantarcticR-cache~ ## fetch the actual data for that layer layer_data &lt;- qa_get(my_layer) ## plot it plot(layer_data[layer_data$MONTH == &quot;October&quot;, ]) ## or add to a SOmap SOmap(trim = -50, border_width = 0.5) SOplot(layer_data[layer_data$MONTH == &quot;October&quot;, ], col = &quot;red&quot;) 10.3.2 antanym See the overview article. "],
["data-visualisation.html", "11 Data visualisation", " 11 Data visualisation Presentation (PDF) "],
["recent-and-past-application-of-sdms.html", "12 Recent and past application of SDMs", " 12 Recent and past application of SDMs Presentation (PDF) "],
["species-distribution-modelling.html", "13 Species distribution modelling 13.1 SDM introduction and review 13.2 SDM outputs 13.3 Calibration 13.4 Exercise scripts", " 13 Species distribution modelling Exercise scripts are at the bottom of this page. 13.1 SDM introduction and review What are species distribution models? Presentation (PDF) 13.1.1 References Araujo and Guisan 2006 Elith 2006 Elith and Leathwick 2009 Guisan and Zimmermann 2000 Peterson 2011 Soberon and Peterson 2005 13.2 SDM outputs Understand and generate SDM outputs. Presentation (PDF) 13.2.1 References Elith et al. 2010 Guillaumot et al. 2018 Ecol &amp; Evol Liu et al. 2013 Torres 2015 13.3 Calibration What you should know before running a model. Presentation (PDF) 13.3.1 References Appendix S1 Elith 2008 Barbet-Massin et al. 2012 Barve 2011 Boria 2014 Brotons 2004 Elith et al. 2008 Elith et al. 2010 Guillaumot et al. 2018 Ecol &amp; Evol Guillaumot et al. 2019 Guillaumot et al. 2018 MEPS Muscarella et al. 2014 Phillips et al. 2009 Wisz and Guisan 2009 13.4 Exercise scripts Exercise - Run your first SDM SDM_EXERCISE.pdf Final practical work Run models, compare different model calibrations and compare your model outputs! SDM_FINAL.pdf Other scripts calibration_BRT.R #### Open environmental data and occurrences (run_yOur_SDM.R file) #--------------------------------------------------------- # compute there the MATRIX_OCC_ENVI variable (1 set) # in the run_yOur_SDM.R file, run until the MyFold definition # for set.seed(1) (j=1) # run the model and compare the predictive deviance according to the set of parameters chosen #============================================================= source(&quot;scripts/Function_gbm.R&quot;) tc=4 # tree complexity lr=0.011 # learning rate bf=0.8 # bag fraction model.res&lt;- gbm.step_v2 (data=MATRIX_OCC_ENVI, gbm.x = 2:ncol(MATRIX_OCC_ENVI), gbm.y = 1, family = &quot;bernoulli&quot;, n.folds=4, fold.vector = MyFold, tree.complexity = tc, learning.rate = lr, bag.fraction =bf) model1&lt;-model.res # tc3 lr0.001 bf=0.8 nbt 10000 model2&lt;-model.res # tc4 lr0.001 bf=0.8 nbt 10000 model3&lt;-model.res # tc4 lr0.005 bf=0.8 nbt 10000 model4&lt;-model.res # tc3 lr0.005 bf=0.8 nbt 10000 model5&lt;-model.res # tc4 lr0.01 bf=0.8 nbt 10000 model6&lt;-model.res # tc4 lr0.008 bf=0.8 nbt 10000 model7&lt;-model.res # tc4 lr0.005 bf=0.9 nbt 10000 model8&lt;-model.res # tc4 lr0.011 bf=0.8 nbt 10000 # GENERATE THE PLOTS tree.list1 &lt;- seq(100, model1$gbm.call$best.trees, by = 100) tree.list2 &lt;- seq(100, model2$gbm.call$best.trees, by = 100) tree.list3 &lt;- seq(100, model3$gbm.call$best.trees, by = 100) tree.list4 &lt;- seq(100, model4$gbm.call$best.trees, by = 100) tree.list5 &lt;- seq(100, model5$gbm.call$best.trees, by = 100) tree.list6 &lt;- seq(100, model6$gbm.call$best.trees, by = 100) tree.list7 &lt;- seq(100, model7$gbm.call$best.trees, by = 100) tree.list8 &lt;- seq(100, model8$gbm.call$best.trees, by = 100) pred1 &lt;- predict.gbm(model1,MATRIX_OCC_ENVI, n.trees = tree.list1, &quot;response&quot;) pred2 &lt;- predict.gbm(model2,MATRIX_OCC_ENVI, n.trees = tree.list2, &quot;response&quot;) pred3 &lt;- predict.gbm(model3,MATRIX_OCC_ENVI, n.trees = tree.list3, &quot;response&quot;) pred4 &lt;- predict.gbm(model4,MATRIX_OCC_ENVI, n.trees = tree.list4, &quot;response&quot;) pred5 &lt;- predict.gbm(model5,MATRIX_OCC_ENVI, n.trees = tree.list5, &quot;response&quot;) pred6 &lt;- predict.gbm(model6,MATRIX_OCC_ENVI, n.trees = tree.list6, &quot;response&quot;) pred7 &lt;- predict.gbm(model7,MATRIX_OCC_ENVI, n.trees = tree.list7, &quot;response&quot;) pred8 &lt;- predict.gbm(model8,MATRIX_OCC_ENVI, n.trees = tree.list8, &quot;response&quot;) graphe.deviance1 &lt;- rep(0,max(tree.list1)/100) for (i in 1:length(graphe.deviance1)) { graphe.deviance1 [i] &lt;- calc.deviance(MATRIX_OCC_ENVI$id, pred1[,i],calc.mean=T) } graphe.deviance2 &lt;- rep(0,max(tree.list2)/100) for (i in 1:length(graphe.deviance2)) { graphe.deviance2 [i] &lt;- calc.deviance(MATRIX_OCC_ENVI$id, pred2[,i],calc.mean=T) } graphe.deviance3 &lt;- rep(0,max(tree.list3)/100) for (i in 1:length(graphe.deviance3)) { graphe.deviance3 [i] &lt;- calc.deviance(MATRIX_OCC_ENVI$id, pred3[,i],calc.mean=T) } graphe.deviance4 &lt;- rep(0,max(tree.list4)/100) for (i in 1:length(graphe.deviance4)) { graphe.deviance4 [i] &lt;- calc.deviance(MATRIX_OCC_ENVI$id, pred4[,i],calc.mean=T) } graphe.deviance5 &lt;- rep(0,max(tree.list5)/100) for (i in 1:length(graphe.deviance5)) { graphe.deviance5 [i] &lt;- calc.deviance(MATRIX_OCC_ENVI$id, pred5[,i],calc.mean=T) } graphe.deviance6 &lt;- rep(0,max(tree.list6)/100) for (i in 1:length(graphe.deviance6)) { graphe.deviance6 [i] &lt;- calc.deviance(MATRIX_OCC_ENVI$id, pred6[,i],calc.mean=T) } graphe.deviance7 &lt;- rep(0,max(tree.list7)/100) for (i in 1:length(graphe.deviance7)) { graphe.deviance7 [i] &lt;- calc.deviance(MATRIX_OCC_ENVI$id, pred7[,i],calc.mean=T) } graphe.deviance8 &lt;- rep(0,max(tree.list8)/100) for (i in 1:length(graphe.deviance8)) { graphe.deviance8 [i] &lt;- calc.deviance(MATRIX_OCC_ENVI$id, pred8[,i],calc.mean=T) } par(mai=c(0.5,0.5,0.5,0.5)) plot(tree.list1,graphe.deviance1,xlim = c(-100,4000), ylim=c(0,1),type=&#39;l&#39;, xlab = &quot;number of trees&quot;,ylab = &quot;predictive deviance&quot;, cex.lab = 1.5) lines(tree.list2,graphe.deviance2,col=&quot;blue&quot;) lines(tree.list3,graphe.deviance3,col=&quot;red&quot;) lines(tree.list4,graphe.deviance4,col=&quot;green&quot;) lines(tree.list5,graphe.deviance5,col=&quot;orange&quot;) lines(tree.list6,graphe.deviance6,col=&quot;pink&quot;) lines(tree.list7,graphe.deviance7,col=&quot;deepskyblue&quot;) lines(tree.list8,graphe.deviance8,col=&quot;purple&quot;) legend(&quot;topright&quot;,legend=c(&quot;tc3 lr0.001 bf=0.8&quot;,&quot;tc4 lr0.001 bf=0.8&quot;,&quot;tc4 lr0.005 bf=0.8 &quot;,&quot;tc3 lr0.005 bf=0.8&quot;,&quot;tc4 lr0.01 bf=0.8&quot;,&quot;tc4 lr0.008 bf=0.8&quot;,&quot;tc4 lr0.005 bf=0.9&quot;,&quot;tc4 lr0.011 bf=0.8&quot;),col=c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;,&quot;green&quot;,&quot;orange&quot;,&quot;pink&quot;,&quot;deepskyblue&quot;,&quot;purple&quot;),pch=16,cex=0.7) clock4_crossValidation.R clock4 &lt;- function(occ, bg.coords){ # the spatial sampling aims at defining areas containing either test or training data # for the CLOCK-4 method, 4 triangle areas are defined, cutting the Southern Ocean circle into 4 equal areas. Three of these 4 areas contain the data that will be used to train the model, the other remaining one, the data that will be used to test the model after predictions # the areas used to train and test the model are randomly defined at each loop step #initialise vectors that are used to generate the spatial sampling structure random_longA &lt;- seq(0,179,1) random_longB &lt;- seq(-180,-1,1) random_longC &lt;- seq(0,180,1) random_longD &lt;- seq(-179,0,1) random_long &lt;- c(random_longA,random_longB,random_longC,random_longD) # sample a number between -180 and 180 to define the random sampling transect tirage &lt;- sample(seq(181,541,1),1) random_long_tirage &lt;- random_long[tirage] random_long_tirage_A1 &lt;- random_long[tirage+90] random_long_tirage_A2 &lt;- random_long[tirage+180] random_long_tirage_A3 &lt;- random_long[tirage-90] #random_long_tirage_A4 &lt;- random_long[tirage-180] ## define training and test groups (composed of both presence and background data) occ.grp &lt;- rep(NA, nrow(occ)) bg.coords.grp &lt;- rep(NA, nrow(bg.coords)) ## define training and test groups (composed of both presence and background data) presence_tot &lt;- occ background_data &lt;- bg.coords training_presences.occ1 &lt;- NA;training_presences.occ3 &lt;-NA training_presences.occ1_part1 &lt;- NA;training_presences.occ3_part1 &lt;-NA training_presences.occ1_part2 &lt;- NA;training_presences.occ3_part2 &lt;-NA training_backgr.occ1 &lt;- NA; training_backgr.occ3 &lt;- NA training_backgr.occ1_part1 &lt;- NA; training_backgr.occ3_part1 &lt;- NA training_backgr.occ1_part2 &lt;- NA; training_backgr.occ3_part2 &lt;- NA training_presences.occ2 &lt;- NA;training_presences.occ4 &lt;- NA training_presences.occ2_part1 &lt;- NA;training_presences.occ4_part1 &lt;- NA training_presences.occ2_part2 &lt;- NA;training_presences.occ4_part2 &lt;- NA training_backgr.occ2 &lt;- NA;training_backgr.occ4 &lt;- NA training_backgr.occ2_part1 &lt;- NA;training_backgr.occ4_part1 &lt;- NA training_backgr.occ2_part2 &lt;- NA;training_backgr.occ4_part2 &lt;- NA #--------------------- # TRAINING PRESENCES #--------------------- ### ZONE 1 #### if (random_long_tirage &lt;= 0 &amp; random_long_tirage_A1 &lt;=0){ # imply that A2 &gt;= 0 and A3&gt;=0 training_presences.occ1 &lt;- which(presence_tot[,1] &gt; random_long_tirage &amp; presence_tot[,1] &lt; random_long_tirage_A1) training_backgr.occ1 &lt;- which(background_data[,1] &gt; random_long_tirage &amp; background_data[,1] &lt; random_long_tirage_A1) } if (random_long_tirage &gt;= 0 &amp; random_long_tirage_A1 &gt;=0){ training_presences.occ1 &lt;- which(presence_tot[,1] &gt; random_long_tirage &amp; presence_tot[,1] &lt; random_long_tirage_A1) training_backgr.occ1 &lt;- which(background_data[,1] &gt; random_long_tirage &amp; background_data[,1] &lt; random_long_tirage_A1) } if (random_long_tirage &gt;= 0 &amp; random_long_tirage_A1 &lt;=0){ training_presences.occ1_part1 &lt;- which(presence_tot[,1] &gt; random_long_tirage &amp; presence_tot[,1] &lt; 180 ) training_presences.occ1_part2 &lt;- which(presence_tot[,1] &gt; -180 &amp; presence_tot[,1] &lt; random_long_tirage_A1 ) training_backgr.occ1_part1 &lt;- which(background_data[,1] &gt; random_long_tirage &amp; background_data[,1] &lt; 180 ) training_backgr.occ1_part2 &lt;- which(background_data[,1] &gt; -180 &amp; background_data[,1] &lt; random_long_tirage_A1 ) } if (random_long_tirage &lt;= 0 &amp; random_long_tirage_A1 &gt;=0){ training_presences.occ1_part1 &lt;- which(presence_tot[,1] &gt; random_long_tirage &amp; presence_tot[,1] &lt; 0 ) training_presences.occ1_part2 &lt;- which(presence_tot[,1] &gt; 0 &amp; presence_tot[,1] &lt; random_long_tirage_A1 ) training_backgr.occ1_part1 &lt;- which(background_data[,1] &gt; random_long_tirage &amp; background_data[,1] &lt; 0 ) training_backgr.occ1_part2 &lt;- which(background_data[,1] &gt; 0 &amp; background_data[,1] &lt; random_long_tirage_A1 ) } ### ZONE 2 #### if (random_long_tirage_A1 &lt;= 0 &amp; random_long_tirage_A2 &lt;=0){ training_presences.occ2 &lt;- which(presence_tot[,1] &gt; random_long_tirage_A1 &amp; presence_tot[,1] &lt; random_long_tirage_A2 ) training_backgr.occ2 &lt;- which(background_data[,1] &gt; random_long_tirage_A1 &amp; background_data[,1] &lt; random_long_tirage_A2) } if (random_long_tirage_A1 &gt;= 0 &amp; random_long_tirage_A2 &gt;=0){ training_presences.occ2 &lt;- which(presence_tot[,1] &gt; random_long_tirage_A1 &amp; presence_tot[,1] &lt; random_long_tirage_A2 ) training_backgr.occ2 &lt;- which(background_data[,1] &gt; random_long_tirage_A1 &amp; background_data[,1] &lt; random_long_tirage_A2) } if (random_long_tirage_A1 &gt;= 0 &amp; random_long_tirage_A2 &lt;=0){ training_presences.occ2_part1 &lt;- which(presence_tot[,1] &gt; random_long_tirage_A1 &amp; presence_tot[,1] &lt; 180) training_presences.occ2_part2 &lt;- which(presence_tot[,1] &gt; -180 &amp; presence_tot[,1] &lt; random_long_tirage_A2) training_backgr.occ2_part1 &lt;- which(background_data[,1] &gt; random_long_tirage_A1 &amp; background_data[,1] &lt; 180) training_backgr.occ2_part2 &lt;- which(background_data[,1] &gt; -180 &amp; background_data[,1] &lt; random_long_tirage_A2) } if (random_long_tirage_A1 &lt;= 0 &amp; random_long_tirage_A2 &gt;=0){ training_presences.occ2_part1 &lt;- which(presence_tot[,1] &gt; random_long_tirage_A1 &amp; presence_tot[,1] &lt; 0 ) training_presences.occ2_part2 &lt;- which(presence_tot[,1] &gt; 0 &amp; presence_tot[,1] &lt; random_long_tirage_A2 ) training_backgr.occ2_part1 &lt;- which(background_data[,1] &gt; random_long_tirage_A1 &amp; background_data[,1] &lt; 0 ) training_backgr.occ2_part2 &lt;- which(background_data[,1] &gt; 0 &amp; background_data[,1] &lt; random_long_tirage_A2 ) } ### ZONE 3 #### if (random_long_tirage_A2 &lt;= 0 &amp; random_long_tirage_A3 &lt;=0){ training_presences.occ3 &lt;- which(presence_tot[,1] &gt; random_long_tirage_A2 &amp; presence_tot[,1] &lt; random_long_tirage_A3 ) training_backgr.occ3 &lt;- which(background_data[,1] &gt; random_long_tirage_A2 &amp; background_data[,1] &lt; random_long_tirage_A3) } if (random_long_tirage_A2 &gt;= 0 &amp; random_long_tirage_A3 &gt;=0){ training_presences.occ3 &lt;- which(presence_tot[,1] &gt; random_long_tirage_A2 &amp; presence_tot[,1] &lt; random_long_tirage_A3 ) training_backgr.occ3 &lt;- which(background_data[,1] &gt; random_long_tirage_A2 &amp; background_data[,1] &lt; random_long_tirage_A3) } if (random_long_tirage_A2 &gt;= 0 &amp; random_long_tirage_A3 &lt;=0){ training_presences.occ3_part1 &lt;- which(presence_tot[,1] &gt; random_long_tirage_A2 &amp; presence_tot[,1] &lt; 180 ) training_presences.occ3_part2 &lt;- which(presence_tot[,1] &gt; -180 &amp; presence_tot[,1] &lt; random_long_tirage_A3 ) training_backgr.occ3_part1 &lt;- which(background_data[,1] &gt; random_long_tirage_A2 &amp; background_data[,1] &lt; 180 ) training_backgr.occ3_part2 &lt;- which(background_data[,1] &gt; -180 &amp; background_data[,1] &lt; random_long_tirage_A3 ) } if (random_long_tirage_A2 &lt;= 0 &amp; random_long_tirage_A3 &gt;=0){ training_presences.occ3_part1 &lt;- which(presence_tot[,1] &gt; random_long_tirage_A2 &amp; presence_tot[,1] &lt; 0 ) training_presences.occ3_part2 &lt;- which(presence_tot[,1] &gt; 0 &amp; presence_tot[,1] &lt; random_long_tirage_A3 ) training_backgr.occ3_part1 &lt;- which(background_data[,1] &gt; random_long_tirage_A2 &amp; background_data[,1] &lt; 0 ) training_backgr.occ3_part2 &lt;- which(background_data[,1] &gt; 0 &amp; background_data[,1] &lt; random_long_tirage_A3 ) } ### set the groups training_presence_grp1_all &lt;- as.vector(na.omit(c(training_presences.occ1,training_presences.occ1_part1,training_presences.occ1_part2))) training_presence_grp2_all &lt;- as.vector(na.omit(c(training_presences.occ2,training_presences.occ2_part1,training_presences.occ2_part2))) training_presence_grp3_all &lt;- as.vector(na.omit(c(training_presences.occ3,training_presences.occ3_part1,training_presences.occ3_part2))) for (i in training_presence_grp1_all){ occ.grp[i] &lt;- 1 } for (i in training_presence_grp2_all){ occ.grp[i] &lt;- 2 } for (i in training_presence_grp3_all){ occ.grp[i] &lt;- 3 } occ.grp[which(is.na(occ.grp))]=4 training_backgr_grp1_all &lt;- as.vector(na.omit(c(training_backgr.occ1,training_backgr.occ1_part1,training_backgr.occ1_part2))) training_backgr_grp2_all &lt;- as.vector(na.omit(c(training_backgr.occ2,training_backgr.occ2_part1,training_backgr.occ2_part2))) training_backgr_grp3_all &lt;- as.vector(na.omit(c(training_backgr.occ3,training_backgr.occ3_part1,training_backgr.occ3_part2))) for (i in training_backgr_grp1_all){ bg.coords.grp[i] &lt;- 1 } for (i in training_backgr_grp2_all){ bg.coords.grp[i] &lt;- 2 } for (i in training_backgr_grp3_all){ bg.coords.grp[i] &lt;- 3 } bg.coords.grp[which(is.na(bg.coords.grp))]=4 out &lt;- list(occ.grp=occ.grp,bg.coords.grp= bg.coords.grp, tirage=tirage) return(out) } Function_gbm.R ### gbm.stepv2 .roc &lt;-function (obsdat, preddat) { # code adapted from Ferrier, Pearce and Watson&#39;s code, by J.Elith # # see: # Hanley, J.A. &amp; McNeil, B.J. (1982) The meaning and use of the area # under a Receiver Operating Characteristic (ROC) curve. # Radiology, 143, 29-36 # # Pearce, J. &amp; Ferrier, S. (2000) Evaluating the predictive performance # of habitat models developed using logistic regression. # Ecological Modelling, 133, 225-245. # this is the non-parametric calculation for area under the ROC curve, # using the fact that a MannWhitney U statistic is closely related to # the area # # in dismo, this is used in the gbm routines, but not elsewhere (see evaluate). if (length(obsdat) != length(preddat)) { stop(&quot;obs and preds must be equal lengths&quot;) } n.x &lt;- length(obsdat[obsdat == 0]) n.y &lt;- length(obsdat[obsdat == 1]) xy &lt;- c(preddat[obsdat == 0], preddat[obsdat == 1]) rnk &lt;- rank(xy) wilc &lt;- ((n.x * n.y) + ((n.x * (n.x + 1))/2) - sum(rnk[1:n.x]))/(n.x * n.y) return(round(wilc, 4)) } .calibration &lt;- function(obs, preds, family = &quot;binomial&quot;) { # # j elith/j leathwick 17th March 2005 # calculates calibration statistics for either binomial or count data # but the family argument must be specified for the latter # a conditional test for the latter will catch most failures to specify # the family # if (family == &quot;bernoulli&quot;) { family &lt;- &quot;binomial&quot; } pred.range &lt;- max(preds) - min(preds) if(pred.range &gt; 1.2 &amp; family == &quot;binomial&quot;) { print(paste(&quot;range of response variable is &quot;, round(pred.range, 2)), sep = &quot;&quot;, quote = F) print(&quot;check family specification&quot;, quote = F) return() } if(family == &quot;binomial&quot;) { pred &lt;- preds + 1e-005 pred[pred &gt;= 1] &lt;- 0.99999 mod &lt;- glm(obs ~ log((pred)/(1 - (pred))), family = binomial) lp &lt;- log((pred)/(1 - (pred))) a0b1 &lt;- glm(obs ~ offset(lp) - 1, family = binomial) miller1 &lt;- 1 - pchisq(a0b1$deviance - mod$deviance, 2) ab1 &lt;- glm(obs ~ offset(lp), family = binomial) miller2 &lt;- 1 - pchisq(a0b1$deviance - ab1$deviance, 1) miller3 &lt;- 1 - pchisq(ab1$deviance - mod$deviance, 1) } else if(family == &quot;poisson&quot;) { mod &lt;- glm(obs ~ log(preds), family = poisson) lp &lt;- log(preds) a0b1 &lt;- glm(obs ~ offset(lp) - 1, family = poisson) miller1 &lt;- 1 - pchisq(a0b1$deviance - mod$deviance, 2) ab1 &lt;- glm(obs ~ offset(lp), family = poisson) miller2 &lt;- 1 - pchisq(a0b1$deviance - ab1$deviance, 1) miller3 &lt;- 1 - pchisq(ab1$deviance - mod$deviance, 1) } calibration.result &lt;- c(mod$coef, miller1, miller2, miller3) names(calibration.result) &lt;- c(&quot;intercept&quot;, &quot;slope&quot;, &quot;testa0b1&quot;, &quot;testa0|b1&quot;, &quot;testb1|a&quot;) return(calibration.result) } ## VI. gbm.step_v2 &lt;- function (data, gbm.x, gbm.y, offset = NULL, fold.vector = NULL, tree.complexity = 1, learning.rate = 0.01, bag.fraction = 0.75, site.weights = rep(1, nrow(data)), var.monotone = rep(0, length(gbm.x)), n.folds = 10, prev.stratify = TRUE, family = &quot;bernoulli&quot;, n.trees = 50, step.size = n.trees, max.trees = 10000, tolerance.method = &quot;auto&quot;, tolerance = 0.001, plot.main = TRUE, plot.folds = FALSE, verbose = TRUE, silent = FALSE, keep.fold.models = FALSE, keep.fold.vector = FALSE, keep.fold.fit = FALSE, ...) { if (!requireNamespace(&quot;gbm&quot;)) { stop(&quot;you need to install the gbm package to run this function&quot;) } requireNamespace(&quot;splines&quot;) if (silent) verbose &lt;- FALSE z1 &lt;- Sys.time() x.data &lt;- data[, gbm.x, drop = FALSE] y.data &lt;- data[, gbm.y] sp.name &lt;- names(data)[gbm.y] if (family == &quot;bernoulli&quot;) { prevalence &lt;- mean(y.data) } n.cases &lt;- nrow(data) n.preds &lt;- length(gbm.x) if (!silent) { cat(&quot;\\n&quot;, &quot;\\n&quot;, &quot;GBM STEP - version 2.9&quot;, &quot;\\n&quot;, &quot;\\n&quot;) cat(&quot;Performing cross-validation optimisation of a boosted regression tree model \\n&quot;) cat(&quot;for&quot;, sp.name, &quot;and using a family of&quot;, family, &quot;\\n&quot;) cat(&quot;Using&quot;, n.cases, &quot;observations and&quot;, n.preds, &quot;predictors \\n&quot;) } if (is.null(fold.vector)) { if (prev.stratify &amp; family == &quot;bernoulli&quot;) { presence.mask &lt;- data[, gbm.y] == 1 absence.mask &lt;- data[, gbm.y] == 0 n.pres &lt;- sum(presence.mask) n.abs &lt;- sum(absence.mask) selector &lt;- rep(0, n.cases) temp &lt;- rep(seq(1, n.folds, by = 1), length = n.pres) temp &lt;- temp[order(runif(n.pres, 1, 100))] selector[presence.mask] &lt;- temp temp &lt;- rep(seq(1, n.folds, by = 1), length = n.abs) temp &lt;- temp[order(runif(n.abs, 1, 100))] selector[absence.mask] &lt;- temp } else { selector &lt;- rep(seq(1, n.folds, by = 1), length = n.cases) selector &lt;- selector[order(runif(n.cases, 1, 100))] } } else { if (length(fold.vector) != n.cases) { stop(&quot;supplied fold vector is of wrong length&quot;) } cat(&quot;loading user-supplied fold vector \\n&quot;) selector &lt;- fold.vector } pred.values &lt;- rep(0, n.cases) cv.loss.matrix &lt;- matrix(0, nrow = n.folds, ncol = 1) training.loss.matrix &lt;- matrix(0, nrow = n.folds, ncol = 1) trees.fitted &lt;- n.trees model.list &lt;- list(paste(&quot;model&quot;, 1:n.folds, sep = &quot;&quot;)) if (is.null(offset)) { gbm.call &lt;- paste(&quot;gbm::gbm(y.subset ~ .,data=x.subset, n.trees = n.trees, interaction.depth = tree.complexity, shrinkage = learning.rate, bag.fraction = bag.fraction, weights = weight.subset, distribution = as.character(family), var.monotone = var.monotone, verbose = FALSE)&quot;, sep = &quot;&quot;) } else { gbm.call &lt;- paste(&quot;gbm::gbm(y.subset ~ . + offset(offset.subset), data=x.subset, n.trees = n.trees, interaction.depth = tree.complexity, shrinkage = learning.rate, bag.fraction = bag.fraction, weights = weight.subset, distribution = as.character(family), var.monotone = var.monotone, verbose = FALSE)&quot;, sep = &quot;&quot;) } n.fitted &lt;- n.trees y_i &lt;- y.data u_i &lt;- sum(y.data * site.weights)/sum(site.weights) u_i &lt;- rep(u_i, length(y_i)) total.deviance &lt;- calc.deviance(y_i, u_i, weights = site.weights, family = family, calc.mean = FALSE) mean.total.deviance &lt;- total.deviance/n.cases tolerance.test &lt;- tolerance if (tolerance.method == &quot;auto&quot;) { tolerance.test &lt;- mean.total.deviance * tolerance } if (!silent) { cat(&quot;creating&quot;, n.folds, &quot;initial models of&quot;, n.trees, &quot;trees&quot;, &quot;\\n&quot;) if (prev.stratify &amp; family == &quot;bernoulli&quot;) { cat(&quot;\\n&quot;, &quot;folds are stratified by prevalence&quot;, &quot;\\n&quot;) } else { cat(&quot;\\n&quot;, &quot;folds are unstratified&quot;, &quot;\\n&quot;) } cat(&quot;total mean deviance = &quot;, round(mean.total.deviance, 4), &quot;\\n&quot;) cat(&quot;tolerance is fixed at &quot;, round(tolerance.test, 4), &quot;\\n&quot;) if (tolerance.method != &quot;fixed&quot; &amp; tolerance.method != &quot;auto&quot;) { stop(&quot;invalid argument for tolerance method - should be auto or fixed&quot;) } } if (verbose) { cat(&quot;ntrees resid. dev.&quot;, &quot;\\n&quot;) } for (i in 1:n.folds) { model.mask &lt;- selector != i pred.mask &lt;- selector == i y.subset &lt;- y.data[model.mask] x.subset &lt;- x.data[model.mask, , drop = FALSE] weight.subset &lt;- site.weights[model.mask] if (!is.null(offset)) { offset.subset &lt;- offset[model.mask] } else { offset.subset &lt;- NULL } model.list[[i]] &lt;- eval(parse(text = gbm.call)) fitted.values &lt;- model.list[[i]]$fit if (!is.null(offset)) { fitted.values &lt;- fitted.values + offset[model.mask] } if (family == &quot;bernoulli&quot;) { fitted.values &lt;- exp(fitted.values)/(1 + exp(fitted.values)) } else if (family == &quot;poisson&quot;) { fitted.values &lt;- exp(fitted.values) } pred.values[pred.mask] &lt;- gbm::predict.gbm(model.list[[i]], x.data[pred.mask, , drop = FALSE], n.trees = n.trees) if (!is.null(offset)) { pred.values[pred.mask] &lt;- pred.values[pred.mask] + offset[pred.mask] } if (family == &quot;bernoulli&quot;) { pred.values[pred.mask] &lt;- exp(pred.values[pred.mask])/(1 + exp(pred.values[pred.mask])) } else if (family == &quot;poisson&quot;) { pred.values[pred.mask] &lt;- exp(pred.values[pred.mask]) } y_i &lt;- y.subset u_i &lt;- fitted.values weight.fitted &lt;- site.weights[model.mask] training.loss.matrix[i, 1] &lt;- calc.deviance(y_i, u_i, weight.fitted, family = family) y_i &lt;- y.data[pred.mask] u_i &lt;- pred.values[pred.mask] weight.preds &lt;- site.weights[pred.mask] cv.loss.matrix[i, 1] &lt;- calc.deviance(y_i, u_i, weight.preds, family = family) } delta.deviance &lt;- 1 cv.loss.values &lt;- apply(cv.loss.matrix, 2, mean) if (verbose) { cat(n.fitted, &quot; &quot;, round(cv.loss.values, 4), &quot;\\n&quot;) } if (!silent) { cat(&quot;now adding trees...&quot;, &quot;\\n&quot;) } j &lt;- 1 while (delta.deviance &gt; tolerance.test &amp; n.fitted &lt; max.trees) { training.loss.matrix &lt;- cbind(training.loss.matrix, rep(0, n.folds)) cv.loss.matrix &lt;- cbind(cv.loss.matrix, rep(0, n.folds)) n.fitted &lt;- n.fitted + step.size trees.fitted &lt;- c(trees.fitted, n.fitted) j &lt;- j + 1 for (i in 1:n.folds) { model.mask &lt;- selector != i pred.mask &lt;- selector == i y.subset &lt;- y.data[model.mask] x.subset &lt;- x.data[model.mask, , drop = FALSE] weight.subset &lt;- site.weights[model.mask] if (!is.null(offset)) { offset.subset &lt;- offset[model.mask] } model.list[[i]] &lt;- gbm::gbm.more(model.list[[i]], weights = weight.subset, step.size) fitted.values &lt;- model.list[[i]]$fit if (!is.null(offset)) { fitted.values &lt;- fitted.values + offset[model.mask] } if (family == &quot;bernoulli&quot;) { fitted.values &lt;- exp(fitted.values)/(1 + exp(fitted.values)) } else if (family == &quot;poisson&quot;) { fitted.values &lt;- exp(fitted.values) } pred.values[pred.mask] &lt;- gbm::predict.gbm(model.list[[i]], x.data[pred.mask, , drop = FALSE], n.trees = n.fitted) if (!is.null(offset)) { pred.values[pred.mask] &lt;- pred.values[pred.mask] + offset[pred.mask] } if (family == &quot;bernoulli&quot;) { pred.values[pred.mask] &lt;- exp(pred.values[pred.mask])/(1 + exp(pred.values[pred.mask])) } else if (family == &quot;poisson&quot;) { pred.values[pred.mask] &lt;- exp(pred.values[pred.mask]) } y_i &lt;- y.subset u_i &lt;- fitted.values weight.fitted &lt;- site.weights[model.mask] training.loss.matrix[i, j] &lt;- calc.deviance(y_i, u_i, weight.fitted, family = family) u_i &lt;- pred.values[pred.mask] y_i &lt;- y.data[pred.mask] weight.preds &lt;- site.weights[pred.mask] cv.loss.matrix[i, j] &lt;- calc.deviance(y_i, u_i, weight.preds, family = family) } cv.loss.values &lt;- apply(cv.loss.matrix, 2, mean) if (j &lt; 5) { if (cv.loss.values[j] &gt; cv.loss.values[j - 1]) { if (!silent) { cat(&quot;restart model with a smaller learning rate or smaller step size...&quot;) } return() } } if (j &gt;= 20) { test1 &lt;- mean(cv.loss.values[(j - 9):j]) test2 &lt;- mean(cv.loss.values[(j - 19):(j - 9)]) delta.deviance &lt;- test2 - test1 } if (verbose) { cat(n.fitted, &quot; &quot;, round(cv.loss.values[j], 4), &quot;\\n&quot;) flush.console() } } training.loss.values &lt;- apply(training.loss.matrix, 2, mean) cv.loss.ses &lt;- rep(0, length(cv.loss.values)) cv.loss.ses &lt;- sqrt(apply(cv.loss.matrix, 2, var))/sqrt(n.folds) y.bar &lt;- min(cv.loss.values) target.trees &lt;- trees.fitted[match(TRUE, cv.loss.values == y.bar)] if (plot.main) { y.min &lt;- min(cv.loss.values - cv.loss.ses) y.max &lt;- max(cv.loss.values + cv.loss.ses) if (plot.folds) { y.min &lt;- min(cv.loss.matrix) y.max &lt;- max(cv.loss.matrix) } plot(trees.fitted, cv.loss.values, type = &quot;l&quot;, ylab = &quot;holdout deviance&quot;, xlab = &quot;no. of trees&quot;, ylim = c(y.min, y.max), ...) abline(h = y.bar, col = 2) lines(trees.fitted, cv.loss.values + cv.loss.ses, lty = 2) lines(trees.fitted, cv.loss.values - cv.loss.ses, lty = 2) if (plot.folds) { for (i in 1:n.folds) { lines(trees.fitted, cv.loss.matrix[i, ], lty = 3) } } abline(v = target.trees, col = 3) title(paste(sp.name, &quot;, d - &quot;, tree.complexity, &quot;, lr - &quot;, learning.rate, sep = &quot;&quot;)) } cv.deviance.stats &lt;- rep(0, n.folds) cv.roc.stats &lt;- rep(0, n.folds) cv.cor.stats &lt;- rep(0, n.folds) cv.calibration.stats &lt;- matrix(0, ncol = 5, nrow = n.folds) if (family == &quot;bernoulli&quot;) { threshold.stats &lt;- rep(0, n.folds) ############################ ## JA editions / 08/02/19 ## ############################ cv.corr.class &lt;- rep(0, n.folds) cv.length &lt;- rep(0, n.folds) tss.stat &lt;- rep(0, n.folds) #################### ## End / 08/02/19 ## #################### } fitted.matrix &lt;- matrix(NA, nrow = n.cases, ncol = n.folds) fold.fit &lt;- rep(0, n.cases) for (i in 1:n.folds) { pred.mask &lt;- selector == i model.mask &lt;- selector != i fits &lt;- gbm::predict.gbm(model.list[[i]], x.data[model.mask, , drop = FALSE], n.trees = target.trees) if (!is.null(offset)) { fits &lt;- fits + offset[model.mask] } if (family == &quot;bernoulli&quot;) { fits &lt;- exp(fits)/(1 + exp(fits)) } else if (family == &quot;poisson&quot;) { fits &lt;- exp(fits) } fitted.matrix[model.mask, i] &lt;- fits fits &lt;- gbm::predict.gbm(model.list[[i]], x.data[pred.mask, , drop = FALSE], n.trees = target.trees) if (!is.null(offset)) fits &lt;- fits + offset[pred.mask] fold.fit[pred.mask] &lt;- fits if (family == &quot;bernoulli&quot;) { fits &lt;- exp(fits)/(1 + exp(fits)) }else if (family == &quot;poisson&quot;) { fits &lt;- exp(fits) } fitted.matrix[pred.mask, i] &lt;- fits y_i &lt;- y.data[pred.mask] u_i &lt;- fitted.matrix[pred.mask, i] weight.preds &lt;- site.weights[pred.mask] cv.deviance.stats[i] &lt;- calc.deviance(y_i, u_i, weight.preds, family = family) cv.cor.stats[i] &lt;- cor(y_i, u_i) if (family == &quot;bernoulli&quot;) { cv.roc.stats[i] &lt;- .roc(y_i, u_i) cv.calibration.stats[i, ] &lt;- .calibration(y_i, u_i, &quot;binomial&quot;) threshold.stats[i] &lt;- approx(ppoints(u_i), sort(u_i, decreasing = T), prevalence)$y ############################ ## JA editions / 08/02/19 ## ############################ c_i &lt;- c(1, 0)[as.factor(u_i &lt;= threshold.stats[i])] tab1 &lt;- table(c_i, y_i) # Correctly classified test data cv.corr.class[i] &lt;- sum(diag(tab1)) / sum(tab1) # % cv / total data cv.length[i] &lt;- table(y_i)[&quot;1&quot;] / table(y.data)[&quot;1&quot;] # TSS # Sensitivity = TP/(TP + FN) # Specificity = TN/(TN + FP) tp &lt;- tab1[&quot;1&quot;, &quot;1&quot;] fn &lt;- tab1[&quot;0&quot;, &quot;1&quot;] tn &lt;- tab1[&quot;0&quot;, &quot;0&quot;] fp &lt;- tab1[&quot;1&quot;, &quot;0&quot;] sen &lt;- tp/(tp + fn) spe &lt;- tn/(tn + fp) # TSS tss.stat[i] &lt;- spe + sen -1 #################### ## End / 08/02/19 ## #################### } if (family == &quot;poisson&quot;) { cv.calibration.stats[i, ] &lt;- .calibration(y_i, u_i, &quot;poisson&quot;) } } fitted.vars &lt;- apply(fitted.matrix, 1, var, na.rm = TRUE) cv.dev &lt;- mean(cv.deviance.stats, na.rm = TRUE) cv.dev.se &lt;- sqrt(var(cv.deviance.stats))/sqrt(n.folds) cv.cor &lt;- mean(cv.cor.stats, na.rm = TRUE) cv.cor.se &lt;- sqrt(var(cv.cor.stats, use = &quot;complete.obs&quot;))/sqrt(n.folds) cv.roc &lt;- 0 cv.roc.se &lt;- 0 if (family == &quot;bernoulli&quot;) { cv.roc &lt;- mean(cv.roc.stats, na.rm = TRUE) cv.roc.se &lt;- sqrt(var(cv.roc.stats, use = &quot;complete.obs&quot;))/sqrt(n.folds) cv.threshold &lt;- mean(threshold.stats, na.rm = T) cv.threshold.se &lt;- sqrt(var(threshold.stats, use = &quot;complete.obs&quot;))/sqrt(n.folds) } cv.calibration &lt;- 0 cv.calibration.se &lt;- 0 if (family == &quot;poisson&quot; | family == &quot;bernoulli&quot;) { cv.calibration &lt;- apply(cv.calibration.stats, 2, mean) cv.calibration.se &lt;- apply(cv.calibration.stats, 2, var) cv.calibration.se &lt;- sqrt(cv.calibration.se)/sqrt(n.folds) } if (is.null(offset)) { gbm.call &lt;- paste(&quot;gbm::gbm(y.data ~ .,data=x.data, n.trees = target.trees, interaction.depth = tree.complexity, shrinkage = learning.rate, bag.fraction = bag.fraction, weights = site.weights, distribution = as.character(family), var.monotone = var.monotone, verbose = FALSE)&quot;, sep = &quot;&quot;) } else { gbm.call &lt;- paste(&quot;gbm::gbm(y.data ~ . + offset(offset),data=x.data, n.trees = target.trees, interaction.depth = tree.complexity, shrinkage = learning.rate, bag.fraction = bag.fraction, weights = site.weights, distribution = as.character(family), var.monotone = var.monotone, verbose = FALSE)&quot;, sep = &quot;&quot;) } if (!silent) { message(&quot;fitting final gbm model with a fixed number of &quot;, target.trees, &quot; trees for &quot;, sp.name) } gbm.object &lt;- eval(parse(text = gbm.call)) best.trees &lt;- target.trees gbm.summary &lt;- summary(gbm.object, n.trees = target.trees, plotit = FALSE) fits &lt;- gbm::predict.gbm(gbm.object, x.data, n.trees = target.trees) if (!is.null(offset)) fits &lt;- fits + offset if (family == &quot;bernoulli&quot;) { fits &lt;- exp(fits)/(1 + exp(fits)) } else if (family == &quot;poisson&quot;) { fits &lt;- exp(fits) } fitted.values &lt;- fits y_i &lt;- y.data u_i &lt;- fitted.values resid.deviance &lt;- calc.deviance(y_i, u_i, weights = site.weights, family = family, calc.mean = FALSE) self.cor &lt;- cor(y_i, u_i) self.calibration &lt;- 0 self.roc &lt;- 0 if (family == &quot;bernoulli&quot;) { deviance.contribs &lt;- (y_i * log(u_i)) + ((1 - y_i) * log(1 - u_i)) residuals &lt;- sqrt(abs(deviance.contribs * 2)) residuals &lt;- ifelse((y_i - u_i) &lt; 0, 0 - residuals, residuals) self.roc &lt;- .roc(y_i, u_i) self.calibration &lt;- .calibration(y_i, u_i, &quot;binomial&quot;) ############################ ## JA editions / 08/02/19 ## ############################ self.th &lt;- approx(ppoints(u_i), sort(u_i, decreasing = T), prevalence)$y #################### ## End / 08/02/19 ## #################### } if (family == &quot;poisson&quot;) { deviance.contribs &lt;- ifelse(y_i == 0, 0, (y_i * log(y_i/u_i))) - (y_i - u_i) residuals &lt;- sqrt(abs(deviance.contribs * 2)) residuals &lt;- ifelse((y_i - u_i) &lt; 0, 0 - residuals, residuals) self.calibration &lt;- .calibration(y_i, u_i, &quot;poisson&quot;) } if (family == &quot;gaussian&quot; | family == &quot;laplace&quot;) { residuals &lt;- y_i - u_i } mean.resid.deviance &lt;- resid.deviance/n.cases z2 &lt;- Sys.time() elapsed.time.minutes &lt;- round(as.numeric(z2 - z1)/60, 2) if (verbose) { cat(&quot;\\n&quot;) cat(&quot;mean total deviance =&quot;, round(mean.total.deviance, 3), &quot;\\n&quot;) cat(&quot;mean residual deviance =&quot;, round(mean.resid.deviance, 3), &quot;\\n&quot;, &quot;\\n&quot;) cat(&quot;estimated cv deviance =&quot;, round(cv.dev, 3), &quot;; se =&quot;, round(cv.dev.se, 3), &quot;\\n&quot;, &quot;\\n&quot;) cat(&quot;training data correlation =&quot;, round(self.cor, 3), &quot;\\n&quot;) cat(&quot;cv correlation = &quot;, round(cv.cor, 3), &quot;; se =&quot;, round(cv.cor.se, 3), &quot;\\n&quot;, &quot;\\n&quot;) if (family == &quot;bernoulli&quot;) { cat(&quot;training data AUC score =&quot;, round(self.roc, 3), &quot;\\n&quot;) cat(&quot;cv AUC score =&quot;, round(cv.roc, 3), &quot;; se =&quot;, round(cv.roc.se, 3), &quot;\\n&quot;, &quot;\\n&quot;) } cat(&quot;elapsed time - &quot;, round(elapsed.time.minutes, 2), &quot;minutes&quot;, &quot;\\n&quot;) } if (n.fitted == max.trees &amp; !silent) { cat(&quot;\\n&quot;, &quot;########### warning ##########&quot;, &quot;\\n&quot;, &quot;\\n&quot;) cat(&quot;maximum tree limit reached - results may not be optimal&quot;, &quot;\\n&quot;) cat(&quot; - refit with faster learning rate or increase maximum number of trees&quot;, &quot;\\n&quot;) } gbm.detail &lt;- list(dataframe = data, gbm.x = gbm.x, predictor.names = names(x.data), gbm.y = gbm.y, response.name = sp.name, offset = offset, family = family, tree.complexity = tree.complexity, learning.rate = learning.rate, bag.fraction = bag.fraction, cv.folds = n.folds, prev.stratification = prev.stratify, max.fitted = n.fitted, n.trees = target.trees, best.trees = target.trees, train.fraction = 1, tolerance.method = tolerance.method, tolerance = tolerance, var.monotone = var.monotone, date = date(), elapsed.time.minutes = elapsed.time.minutes) training.stats &lt;- list(null = total.deviance, mean.null = mean.total.deviance, resid = resid.deviance, mean.resid = mean.resid.deviance, correlation = self.cor, discrimination = self.roc, calibration = self.calibration) cv.stats &lt;- list(deviance.mean = cv.dev, deviance.se = cv.dev.se, correlation.mean = cv.cor, correlation.se = cv.cor.se, discrimination.mean = cv.roc, discrimination.se = cv.roc.se, calibration.mean = cv.calibration, calibration.se = cv.calibration.se) if (family == &quot;bernoulli&quot;) { cv.stats$cv.threshold &lt;- cv.threshold cv.stats$cv.threshold.se &lt;- cv.threshold.se } gbm.object$gbm.call &lt;- gbm.detail gbm.object$fitted &lt;- fitted.values gbm.object$fitted.vars &lt;- fitted.vars gbm.object$residuals &lt;- residuals gbm.object$contributions &lt;- gbm.summary gbm.object$self.statistics &lt;- training.stats gbm.object$cv.statistics &lt;- cv.stats gbm.object$weights &lt;- site.weights gbm.object$trees.fitted &lt;- trees.fitted gbm.object$training.loss.values &lt;- training.loss.values gbm.object$cv.values &lt;- cv.loss.values gbm.object$cv.loss.ses &lt;- cv.loss.ses gbm.object$cv.loss.matrix &lt;- cv.loss.matrix gbm.object$cv.roc.matrix &lt;- cv.roc.stats ############################ ## JA editions / 08/02/19 ## ############################ gbm.object$cv.cor.matrix &lt;- cv.cor.stats gbm.object$cv.th.matrix &lt;- threshold.stats gbm.object$cv.corr.class &lt;- cv.corr.class gbm.object$self.th &lt;- self.th gbm.object$cv.length &lt;- cv.length gbm.object$tss.cv &lt;- tss.stat #################### ## End / 08/02/19 ## #################### if (keep.fold.models) { gbm.object$fold.models &lt;- model.list } else { gbm.object$fold.models &lt;- NULL } if (keep.fold.vector) { gbm.object$fold.vector &lt;- selector } else { gbm.object$fold.vector &lt;- NULL } if (keep.fold.fit) { gbm.object$fold.fit &lt;- fold.fit } else { gbm.object$fold.fit &lt;- NULL } return(gbm.object) } ## II. ## https://github.com/cran/ENMeval/tree/master/R ######################################################### ######### MAKE BLOCK EVALUATION GROUPS ############# ######################################################### get.block &lt;- function(occ, bg.coords){ # SPLIT OCC POINTS INTO FOUR SPATIAL GROUPS noccs &lt;- nrow(occ) n1 &lt;- ceiling(nrow(occ)/2) n2 &lt;- floor(nrow(occ)/2) n3 &lt;- ceiling(n1/2) n4 &lt;- ceiling(n2/2) grpA &lt;- occ[order(occ[, 2]),][1:n1,] grpB &lt;- occ[rev(order(occ[, 2])),][1:n2,] grp1 &lt;- grpA[order(grpA[, 1]),][1:(n3),] grp2 &lt;- grpA[!rownames(grpA)%in%rownames(grp1),] grp3 &lt;- grpB[order(grpB[, 1]),][1:(n4),] grp4 &lt;- grpB[!rownames(grpB)%in%rownames(grp3),] # SPLIT BACKGROUND POINTS BASED ON SPATIAL GROUPS bvert &lt;- mean(max(grp1[, 1]), min(grp2[, 1])) + 0.5 tvert &lt;- mean(max(grp3[, 1]), min(grp4[, 1])) + 0.5 horz &lt;- mean(max(grpA[, 2]), min(grpB[, 2])) + 0.5 bggrp1 &lt;- bg.coords[bg.coords[, 2] &lt;= horz &amp; bg.coords[, 1]&lt;bvert,] bggrp2 &lt;- bg.coords[bg.coords[, 2] &lt; horz &amp; bg.coords[, 1]&gt;=bvert,] bggrp3 &lt;- bg.coords[bg.coords[, 2] &gt; horz &amp; bg.coords[, 1]&lt;=tvert,] bggrp4 &lt;- bg.coords[bg.coords[, 2] &gt;= horz &amp; bg.coords[, 1]&gt;tvert,] rr &lt;- bg.coords[bg.coords[, 2] &gt; horz &amp; bg.coords[, 1] &lt;= bvert &amp; bg.coords[, 1] &gt;= tvert,] r &lt;- data.frame() if (nrow(grp1) &gt; 0) grp1$grp &lt;- 1; r &lt;- rbind(r, grp1) if (nrow(grp2) &gt; 0) grp2$grp &lt;- 2; r &lt;- rbind(r, grp2) if (nrow(grp3) &gt; 0) grp3$grp &lt;- 3; r &lt;- rbind(r, grp3) if (nrow(grp4) &gt; 0) grp4$grp &lt;- 4; r &lt;- rbind(r, grp4) occ.grp &lt;- r[order(as.numeric(rownames(r))),]$grp bgr &lt;- data.frame() if (nrow(bggrp1) &gt; 0) bggrp1$grp &lt;- 1; bgr &lt;- rbind(bgr, bggrp1) if (nrow(bggrp2) &gt; 0) bggrp2$grp &lt;- 2; bgr &lt;- rbind(bgr, bggrp2) if (nrow(bggrp3) &gt; 0) bggrp3$grp &lt;- 3; bgr &lt;- rbind(bgr, bggrp3) if (nrow(bggrp4) &gt; 0) bggrp4$grp &lt;- 4; bgr &lt;- rbind(bgr, bggrp4) bg.grp &lt;- bgr[order(as.numeric(rownames(bgr))),]$grp out &lt;- list(occ.grp=occ.grp, bg.grp=bg.grp) return(out) } ## III. # Fonction pour calculer les nouveaux coordonnées issues d&#39;une rotation d&#39;un nuage de points rota &lt;- function(matxy = MyMat, theta = pi/3, center = center){ matxy &lt;- as.matrix(matxy) colnames(matxy) &lt;- c(&quot;x&quot;, &quot;y&quot;) #3. define a 60 degree counter-clockwise rotation matrix R &lt;- rbind(c(cos(theta), -sin(theta)), c(sin(theta), cos(theta))) # do the rotation... s &lt;- matxy #- center # shift points in the plane so that the center of rotation is at the origin for(hh in 1:nrow(matxy)){ s[hh, ] &lt;- matxy[hh,] - center } so &lt;- cbind(x = s[,&quot;x&quot;] * R[1,1] + s[,&quot;y&quot;] * R[1,2], y = s[,&quot;x&quot;] * R[2,1] + s[,&quot;y&quot;] * R[2,2]) #so = R %*% s # apply the rotation about the origin #vo = so + center # shift again so the origin goes back to the desired center of rotation vo &lt;- matxy #- center # shift points in the plane so that the center of rotation is at the origin for(hh in 1:nrow(matxy)){ vo[hh, ] &lt;- so[hh,] + center } # this can be done in one line as: # vo = R*(v - center) + center return(vo) } run_yOur_SDM.R library(ncdf4) library(raster) library(dismo) library(gbm) #### Open occurrence records #### #--------------------------------- occ.sterechinus &lt;- read.csv(&quot;data/occurrences_sterechinus.csv&quot;, header=T, sep=&quot;;&quot;) # head(occ.sterechinus) #### Open Environmental descriptors layers and stack them together #### #----------------------------------------------------------------------- depth &lt;- raster(&quot;data/environmental_layers/depth.nc&quot;) sediments &lt;- raster(&quot;data/environmental_layers/sediments.nc&quot;) seafloor_temp_2005_2012_max &lt;- raster(&quot;data/environmental_layers/seafloor_temp_2005_2012_max.nc&quot;) POC_2005_2012_max &lt;- raster(&quot;data/environmental_layers/POC_2005_2012_max.nc&quot;) seafloor_current_speed &lt;- raster(&quot;data/environmental_layers/seafloor_current_speed.nc&quot;) predictors_stack &lt;- stack(depth,sediments,seafloor_temp_2005_2012_max,POC_2005_2012_max,seafloor_current_speed) ## have a look at your descriptors properties #.............................................. #predictors_stack #plot(predictors_stack) # have a look at the distribution of occurrences #................................................. #plot(depth) #points(occ.sterechinus[,c(2,1)], pch=20) # longitude first, latitude second #-------------------------------------------------------------------------------------------------------------- # Open the KDE layer of sampling effort, on which the background data will be sampled (by weighting) # The KDE (Kernel Density Estimation) is a statistical tool that helps to measure the probability of finding # an occurrencce on each pixel, according to the set of benthic records sampled in the entire Southern Ocean # (update from the Biogeographic Atlas of the Southern Ocean) #-------------------------------------------------------------------------------------------------------------- KDE &lt;- raster(&quot;data/KDE.asc&quot;) # on this layer, the continents are defined by pixels containing NA values # it enables R to recognise later on the areas where the background data shoud # not be sampled #----------------------------------------------------------------------------------------- #### Set up #------------- cv.boot &lt;- 2 # number of replicates (# of times models are launched/replicated) source(&quot;scripts/Function_gbm.R&quot;) #### Stack of empty data and matrices to fill #---------------------------------------------------------- stack.pred&lt;-subset(predictors_stack,1);values(stack.pred)&lt;-NA #testvaluesp&lt;-rep(NA,nrow(fichier_data)) ; testvaluesa &lt;- testvaluesp model_stats &lt;- matrix(NA, 6, cv.boot*4,dimnames = list(c(&quot;AUC&quot;, &quot;COR&quot;, &quot;TSS&quot;, &quot;maxSSS&quot;, &quot;valid_test_data&quot;,&quot;prop_test&quot;), NULL)) # Stores the contribution contTr &lt;- matrix(NA, dim(predictors_stack)[3], cv.boot, dimnames = list(names(predictors_stack), NULL)) n.seed &lt;- seq(1,60,1)[-c(4,5,17,28,32,41,45,46,48,53)] # controls and fixes the random sampling (to compare results more accurately) for (j in 1:cv.boot){ #------------------------------------------------------------ #### create the matrix of occurrence-environment #------------------------------------------------ envi.presences &lt;- unique(extract (predictors_stack,occ.sterechinus[,c(2,1)])) presence.data &lt;- occ.sterechinus[-which(duplicated(extract(predictors_stack,occ.sterechinus[,c(2,1)]))),c(2,1)]; colnames(presence.data)&lt;- c(&quot;longitude&quot;,&quot;latitude&quot;) # the function &#39;unique&#39; enables to remove the duplicates that may be contained in the dataset (occurrences found in a same pixel); &#39;duplicated&#39; aims at spotting which of these rows are similar #head(envi.presences) # the presence data will be associated to ID=1 set.seed(n.seed[j]) # sampling of background data : in the loop, changes at each replicate # 1000 background data are randomly sampled in the environment, according to the weighting scheme of the KDE layer background_data &lt;- xyFromCell(KDE, sample(which(!is.na(values(KDE))), 200, prob=values(KDE)[!is.na(values(KDE))])) colnames(background_data) &lt;- colnames(presence.data) # extract environmental conditions where the background data are sampled envi.background &lt;- extract(predictors_stack,background_data) # the background data will be associated to ID=0 # Initialise the matrix containing presence, background data and the environmental values associated id&lt;-0;sdmdata.unique&lt;-0; id&lt;-c(rep(1,nrow(envi.presences)),rep(0,nrow(envi.background))) MATRIX_OCC_ENVI&lt;-data.frame(cbind(id,rbind(envi.presences,envi.background))) #head(MATRIX_OCC_ENVI) # Split of the occurrence-background dataset into folds of test/training data (spatially segregated) dat1 &lt;- rbind(cbind(background_data, Isp=rep(0,nrow(background_data))), cbind(presence.data,Isp=rep(1,nrow(presence.data)))); colnames(dat1)&lt;- c(&quot;longitude&quot;,&quot;latitude&quot;,&quot;Isp&quot;) #tail(dat1) idP &lt;- which(dat1$Isp == 1) # id of presence data to split them MyFold &lt;- rep(NA, nrow(dat1)) # an empty box to store the group of the data (1 to 4 afterwards) source(&quot;scripts/clock4_crossValidation.R&quot;) clock4F &lt;- clock4(dat1[idP, c(&quot;longitude&quot;, &quot;latitude&quot;)], dat1[-idP, c(&quot;longitude&quot;, &quot;latitude&quot;)]) # Extracts the folds MyFold[idP] &lt;- clock4F$occ.grp MyFold[-idP] &lt;- clock4F$bg.coords.grp plot(dat1[,c(&quot;longitude&quot;, &quot;latitude&quot;)], pch = 20, col = c(&quot;red&quot;, &quot;blue&quot;,&quot;black&quot;,&quot;purple&quot;)[as.factor(MyFold)]) #-------------------------- #### launch the model ! #-------------------------- model.res&lt;- gbm.step_v2 (data=MATRIX_OCC_ENVI, gbm.x = 2:ncol(MATRIX_OCC_ENVI), gbm.y = 1, family = &quot;bernoulli&quot;, n.folds=4, fold.vector = MyFold, tree.complexity = 3, learning.rate = 0.015, bag.fraction =0.5) #------------------------------------------------------------ #### Extract data and model outputs #------------------------------------------------------------ # Predictions p&lt;-predict(predictors_stack,model.res,n.trees=model.res$gbm.call$best.trees,type=&quot;response&quot;, na.rm=F) stack.pred&lt;-stack(stack.pred,p) # stack all the maps replicates ######## ## CV ## (= on CV folds) ######## j_cv &lt;- ((j-1) * 4+1):(j*4) # to count the folds model_stats[&quot;AUC&quot;, j_cv] &lt;- model.res$cv.roc.matrix model_stats[&quot;COR&quot;, j_cv] &lt;- model.res$cv.cor.matrix model_stats[&quot;TSS&quot;, j_cv] &lt;- model.res$tss.cv model_stats[&quot;maxSSS&quot;, j_cv] &lt;- model.res$cv.th.matrix ## correctly classified test data model_stats[&quot;valid_test_data&quot;, j_cv] &lt;- model.res$cv.corr.class*100 model_stats[&quot;prop_test&quot;, j_cv] &lt;- model.res$cv.length*100 # Get contributions RI &lt;- summary(model.res, plotit = F) # extract the contribution contTr[match(RI$var, rownames(contTr)), j] &lt;- RI[,&quot;rel.inf&quot;] } #------------------------ #### Maps of predictions #------------------------ mean_stack &lt;- raster::calc(stack.pred, mean, na.rm=T); mean_stack &lt;- mask(mean_stack, depth) #sd_stack &lt;- raster::calc(stack.pred,sd, na.rm=T); sd_stack &lt;- mask(sd_stack, depth) # you can plot the results #continent &lt;- read.csv(&quot;data/worldmap.csv&quot;) # add continents lines #plot(mean_stack) ; points(continent, type=&quot;l&quot;) # this is an approximate map, if you want to have a nicer representation, you can # export the ascii document and open it on another software such as Qgis or other # writeRaster(mean_stack, &quot;results/mean_raster.asc&quot;) # writeRaster(sd_stack, &quot;results/sd_raster.asc&quot;) #--------------------- #### Model statistics #--------------------- ecM &lt;- apply(model_stats, 1, mean, na.rm=T) ecSD &lt;- apply(model_stats, 1, sd, na.rm=T) ecTot &lt;- paste(round(ecM, 3), round(ecSD, 3), sep = &quot; ± &quot;) names(ecTot) &lt;- names(ecM) ResF &lt;- data.frame(c(ecTot[&quot;AUC&quot;],ecTot[&quot;COR&quot;],ecTot[&quot;TSS&quot;],ecTot[&quot;maxSSS&quot;], ecTot[&quot;valid_test_data&quot;], ecTot[&quot;prop_test&quot;])) rownames(ResF) &lt;- c(&quot;AUC&quot;,&quot;COR&quot;,&quot;TSS&quot;, &quot;maxSSS&quot;, &quot;Correctly classified test data&quot;, &quot;Test data (% of total dataset)&quot;) ; colnames(ResF) &lt;- &quot;Model average statistics&quot; # matrix of raw results #model_stats # average values #ResF # same, you can export the results #write.csv(model_stats,&quot;results/model_stats.csv&quot;)) ## Contribution of environmental descriptors # raw data #contTr # calculate the average CtM &lt;- apply(contTr, 1, mean) CtSD &lt;- apply(contTr, 1, sd) CtTot &lt;- paste(round(CtM, 3), round(CtSD, 3), sep = &quot; ± &quot;) names(CtTot) &lt;- names(CtM) CtTot &lt;- data.frame(CtTot) ; colnames(CtTot) &lt;- &quot;Contribution (%) of environmental descriptors to the model&quot; #write.csv(CtTot, &quot;results/avg_contribution.csv&quot;) ####--------------------------------------------------------------------------- ####--------------------------------------------------------------------------- ### CALCULATE EXTRAPOLATION # Multivariate Environmental Similarity Surface (Elith et al. 2010) # envi.presences &lt;- unique(extract (predictors_stack,occ.sterechinus[,c(2,1)])) # x &lt;- dismo::mess(predictors_stack, na.omit(envi.presences)) # # y &lt;- x; values(y)&lt;- values(x)&gt;0 # refers to Elith et al. (2010): when the calculated MESS values are negative, it means that it is extrapolating (outside of boundaries) # y &lt;- reclassify(y,cbind(FALSE,0)) # extrapolation area # y &lt;- reclassify(y,cbind(TRUE,1)) # non extrapolation, inside the boundaries of calibration # # plot(y) ####--------------------------------------------------------------------------- ####--------------------------------------------------------------------------- "],
["sdm-algorithms.html", "14 SDM algorithms 14.1 Exercises - Practice with models", " 14 SDM algorithms Introduction to different SDM algorithms. Presentation (PDF) 14.1 Exercises - Practice with models For this exercise, we’re going to play with a superhero dataset! Although this is an SDM focused course, we don’t want to spoil the surprises coming when you start playing with the environmental data. We want to run models for two questions: CLASSIFICATION: Is there a bias towards alignment? (i.e. can we predict if a character will be good or bad) REGRESSION: Are men stronger than women in the comics? (i.e., can we predict total power, and is Gender one of the most important predictors? Or is there something else that predicts total power) As per the slides, we’re going to do a few things: Explore and clean our data Use one-hot-encoding to help deal with many categories Build a single decision tree and visualise it. Re-build the tree with different settings Run a model with gbm.step and visualise the train/test plot, as well as the variable importance plot Run the same model with random forests and look at the differences in the outputs Pick either the classification or regression problem. Solution for the classification problem are included in this document if you need a poke in the right direction. We run the GBM model ONLY for the regression problem here, but you could try out with RF if you wanted Let’s start by loading libraries library(tidyverse) ## Isn&#39;t tidyverse awesome? library(dismo) ## This package contains the gbm.step command library(randomForest) ## This package contains the randomForest library(rpart) ## This package contains a series of useful decision tree functions library(rpart.plot) library(gbm) 14.1.1 Load and clean the data Now that we’ve got some libraries ready, let’s load and clean our SUPERHERO dataset in preparation for our modelling. ## Parsed with column specification: ## cols( ## .default = col_character(), ## `Unnamed: 0` = col_double(), ## Intelligence = col_double(), ## Strength = col_double(), ## Speed = col_double(), ## Durability = col_double(), ## Power = col_double(), ## Combat = col_double(), ## TotalPower = col_double() ## ) ## See spec(...) for full column specifications. Supes &lt;- readr::read_csv(&#39;SuperheroDataset.csv&#39;) For the purposes of this exercise, we’re going to use the following columns: Name, Intelligence, Strength, Speed, Durability, Power, Combat, Gender, Race, Creator, Alignment, Total Power. Use the tidyverse to extract those columns. SuperData &lt;- Supes %&gt;% dplyr::select(Name, Intelligence:Combat, Alignment:Race, Creator, TotalPower) 14.1.1.1 Exploring the data If you explore the data a little bit, you’ll be able to see some of the interesting quirks of the dataset. Use ggplot2 (or base plot if you’re feeling lazy ;) ). SuperData %&gt;% filter(!is.na(Intelligence))%&gt;% ggplot(aes(x=Intelligence,y=Strength)) + geom_point() + geom_smooth(method=lm) + theme_bw() ## Looking at the line, it seems like the problem is that there are some extreme low values ## of intelligence. Outliers perhaps?? Try to find out who those values belong to, and ## that may help you determine if they should be included in the analysis SuperData %&gt;% filter(!is.na(Combat))%&gt;% ggplot(aes(x=Combat,y=Strength)) + geom_point() + geom_smooth(method=lm) + theme_bw() ## We can also filter the data conditionally to explore the dataset with specific classes, e.g.: SuperData %&gt;% filter(!is.na(Combat),Gender==&#39;Female&#39;)%&gt;% ggplot(aes(x=Combat,y=Strength)) + geom_point() + geom_smooth(method=lm) + theme_bw() library(GGally) ## This is a great library for exploring data quickly! Rather than coding everything bit by bit quant_df &lt;- SuperData %&gt;% dplyr::select(Intelligence:Gender,TotalPower) ggpairs(quant_df, progress = FALSE) Note that ‘total power’ is highly correlated with strength, speed, durability and power. This could impact how we interpret the regression problem later as these variables may wash out the signal we’re trying to explore. Not a bad thing if we’re just trying to make predictions! But if we’re trying to answer a specific question, it could ‘muddy the waters’. If you have checked out some of the variables, you might notice two things: Race and Creator have too many categories! There are a number of Creators that are only used once, or may not be appropriate to include. length(unique(SuperData$Race)) ## Number of unique levels in Race ## [1] 63 length(unique(SuperData$Creator)) ## Same only for creator ## [1] 23 ## Let&#39;s just check out Creator: unique(SuperData$Creator) ## [1] &quot;Dark Horse Comics&quot; &quot;DC Comics&quot; &quot;George Lucas&quot; ## [4] &quot;HarperCollins&quot; &quot;Ian Fleming&quot; &quot;Icon Comics&quot; ## [7] &quot;IDW Publishing&quot; &quot;Image Comics&quot; &quot;J. K. Rowling&quot; ## [10] &quot;J. R. R. Tolkien&quot; &quot;Marvel Comics&quot; &quot;Mattel&quot; ## [13] &quot;Microsoft&quot; &quot;NBC - Heroes&quot; &quot;Rebellion&quot; ## [16] &quot;Shueisha&quot; &quot;Sony Pictures&quot; &quot;South Park&quot; ## [19] &quot;Star Trek&quot; &quot;SyFy&quot; &quot;Team Epic TV&quot; ## [22] &quot;Universal Studios&quot; &quot;Wildstorm&quot; SuperData %&gt;% count(Creator) ## # A tibble: 23 x 2 ## Creator n ## &lt;chr&gt; &lt;int&gt; ## 1 Dark Horse Comics 19 ## 2 DC Comics 219 ## 3 George Lucas 15 ## 4 HarperCollins 6 ## 5 Ian Fleming 1 ## 6 Icon Comics 4 ## 7 IDW Publishing 4 ## 8 Image Comics 14 ## 9 J. K. Rowling 1 ## 10 J. R. R. Tolkien 1 ## 11 Marvel Comics 395 ## 12 Mattel 2 ## 13 Microsoft 1 ## 14 NBC - Heroes 19 ## 15 Rebellion 1 ## 16 Shueisha 4 ## 17 Sony Pictures 2 ## 18 South Park 1 ## 19 Star Trek 6 ## 20 SyFy 5 ## 21 Team Epic TV 5 ## 22 Universal Studios 1 ## 23 Wildstorm 4 There are many classes here that may not be appropriate to include. For example, creator Ian Fleming (creator of James Bond) only has one character. This dataset also has Star Trek characters. So, we decide here to filter out the appropriate classes (keeping Marvel Comics, DC Comics, Dark Horse Comics, and Image Comics). Note the usage of %in% FilteredSupes &lt;- SuperData %&gt;% dplyr::filter(Creator %in% c(&#39;Marvel Comics&#39;,&#39;DC Comics&#39;,&#39;Dark Horse Comics&#39;,&#39;Image Comics&#39;)) %&gt;% dplyr::filter(!is.na(Intelligence)) ## we do this last part to remove NA data for cleanliness, though trees don&#39;t require you to do this! FilteredSupes ## # A tibble: 586 x 12 ## Name Intelligence Strength Speed Durability Power Combat ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abe Sapien 95 30 35 65 100 85 ## 2 Alien 80 30 45 90 60 60 ## 3 Angel 90 30 60 90 100 75 ## 4 Buffy 85 30 45 70 50 60 ## 5 Dash 65 15 95 60 20 30 ## 6 Elastigirl 85 35 35 95 55 70 ## 7 Hellboy 85 55 25 95 75 75 ## 8 Jack-Jack 35 35 70 80 100 10 ## 9 Jason Voorhees 85 35 25 100 100 50 ## 10 Liz Sherman 55 35 30 45 75 50 ## 11 Mr Incredible 80 85 35 95 30 40 ## 12 Predator 85 30 25 85 100 90 ## 13 T-1000 90 35 35 100 100 75 ## 14 T-800 90 35 20 60 75 65 ## 15 T-850 90 80 25 90 85 75 ## 16 T-X 90 65 30 85 100 80 ## 17 Violet Parr 80 10 15 50 80 15 ## 18 Abin Sur 80 90 55 65 100 65 ## 19 Adam Strange 85 10 35 40 40 50 ## 20 Alfred Pennyworth 85 10 20 10 10 55 ## 21 Amazo 85 100 85 100 100 100 ## 22 Animal Man 80 50 50 85 75 80 ## 23 Anti-Monitor 95 100 50 100 100 90 ## 24 Aquababy 60 20 15 15 40 15 ## 25 Aqualad 85 45 45 75 90 60 ## 26 Aquaman 95 85 80 80 100 80 ## 27 Ares 90 100 75 100 100 100 ## 28 Arsenal 80 55 60 60 65 85 ## 29 Atlas 85 100 45 100 30 80 ## 30 Atom 90 95 75 95 90 85 ## Alignment Gender Race Creator TotalPower ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 good Male Icthyo Sapien Dark Horse Comics 410 ## 2 bad Male Xenomorph XX121 Dark Horse Comics 365 ## 3 good Male Vampire Dark Horse Comics 445 ## 4 good Female Human Dark Horse Comics 340 ## 5 good Male Human Dark Horse Comics 285 ## 6 good Female Human Dark Horse Comics 375 ## 7 good Male Demon Dark Horse Comics 410 ## 8 good Male Human Dark Horse Comics 330 ## 9 bad Male Human Dark Horse Comics 395 ## 10 good Female Human Dark Horse Comics 290 ## 11 good Male Human Dark Horse Comics 365 ## 12 bad Male Yautja Dark Horse Comics 415 ## 13 bad Male Android Dark Horse Comics 435 ## 14 bad Male Cyborg Dark Horse Comics 345 ## 15 bad Male Cyborg Dark Horse Comics 445 ## 16 bad Female Cyborg Dark Horse Comics 450 ## 17 good Female Human Dark Horse Comics 250 ## 18 good Male Ungaran DC Comics 455 ## 19 good Male Human DC Comics 260 ## 20 good Male Human DC Comics 190 ## 21 bad Male Android DC Comics 570 ## 22 good Male Human DC Comics 420 ## 23 bad Male God / Eternal DC Comics 535 ## 24 good Male Human DC Comics 165 ## 25 good Male Atlantean DC Comics 400 ## 26 good Male Atlantean DC Comics 520 ## 27 neutral Male God / Eternal DC Comics 565 ## 28 good Male Human DC Comics 405 ## 29 bad Male God / Eternal DC Comics 440 ## 30 good Male Human DC Comics 530 ## # ... with 556 more rows ## How many races do we now have?? length(unique(FilteredSupes$Race)) ## [1] 53 Now let’s explore this again. quant_df &lt;- FilteredSupes %&gt;% dplyr::select(Intelligence:Gender,TotalPower) ggpairs(quant_df, progress = FALSE) Not much has really changed, as we have not removed too many rows, which is good thing because it means that those data we removed won’t impact the final conclusions in any major way. This just simplifies our data cleaning for this particular exercise. However, we still have lots of ‘Race’ categories. FilteredSupes %&gt;% count(Race) ## # A tibble: 53 x 2 ## Race n ## &lt;chr&gt; &lt;int&gt; ## 1 Alien 8 ## 2 Amazon 2 ## 3 Android 8 ## 4 Animal 2 ## 5 Asgardian 5 ## 6 Atlantean 5 ## 7 Bizarro 1 ## 8 Bolovaxian 1 ## 9 Clone 1 ## 10 Cosmic Entity 4 ## 11 Cyborg 9 ## 12 Czarnian 1 ## 13 DemiHumanGod 2 ## 14 Demon 6 ## 15 Eternal 3 ## 16 Flora Colossus 1 ## 17 Frost Giant 2 ## 18 God / Eternal 16 ## 19 Gorilla 1 ## 20 Human 384 ## 21 Human / Altered 2 ## 22 Human / Cosmic 2 ## 23 Human / Radiation 12 ## 24 HumanHumanInhuman 1 ## 25 HumanHumanKree 2 ## 26 HumanHumanSpartoi 1 ## 27 HumanHumanVuldarian 1 ## 28 Icthyo Sapien 1 ## 29 Inhuman 4 ## 30 Kakarantharaian 1 ## # ... with 23 more rows Looking at the breakdown of Race, there are many with a count of 1. These can cause some issues when trying to do any cross validation… so we should remove them. FilteredSupes &lt;- FilteredSupes %&gt;% add_count(Race) %&gt;% filter(n &gt; 1) FilteredSupes %&gt;% count(Race) ## # A tibble: 25 x 2 ## Race n ## &lt;chr&gt; &lt;int&gt; ## 1 Alien 8 ## 2 Amazon 2 ## 3 Android 8 ## 4 Animal 2 ## 5 Asgardian 5 ## 6 Atlantean 5 ## 7 Cosmic Entity 4 ## 8 Cyborg 9 ## 9 DemiHumanGod 2 ## 10 Demon 6 ## 11 Eternal 3 ## 12 Frost Giant 2 ## 13 God / Eternal 16 ## 14 Human 384 ## 15 Human / Altered 2 ## 16 Human / Cosmic 2 ## 17 Human / Radiation 12 ## 18 HumanHumanKree 2 ## 19 Inhuman 4 ## 20 Kryptonian 7 ## 21 Metahuman 2 ## 22 Mutant 57 ## 23 New God 3 ## 24 Symbiote 9 ## 25 Vampire 2 14.1.2 One hot encoding! Race has MANY categories! This COULD cause the trees to get swamped by this predictor. Also, by one-hot encoding, we can keep data that would otherwise get thrown out if just filtering out categories. For example, if we removed the ‘cyborg’ category by way of filtering, those character data would be removed as well (they might be useful). One hot encoding turns each category into a binary variable. Can be done easily in tidyverse! OOEdata &lt;- FilteredSupes %&gt;% separate_rows(Race)%&gt;% mutate(count = 1) %&gt;% spread(Race, count, fill = 0, sep = &quot;_&quot;) OOEdata ## # A tibble: 558 x 38 ## Name Intelligence Strength Speed Durability Power Combat ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Angel 90 30 60 90 100 75 ## 2 Buffy 85 30 45 70 50 60 ## 3 Dash 65 15 95 60 20 30 ## 4 Elastigirl 85 35 35 95 55 70 ## 5 Hellboy 85 55 25 95 75 75 ## 6 Jack-Jack 35 35 70 80 100 10 ## 7 Jason Voorhees 85 35 25 100 100 50 ## 8 Liz Sherman 55 35 30 45 75 50 ## 9 Mr Incredible 80 85 35 95 30 40 ## 10 T-1000 90 35 35 100 100 75 ## 11 T-800 90 35 20 60 75 65 ## 12 T-850 90 80 25 90 85 75 ## 13 T-X 90 65 30 85 100 80 ## 14 Violet Parr 80 10 15 50 80 15 ## 15 Adam Strange 85 10 35 40 40 50 ## 16 Alfred Pennyworth 85 10 20 10 10 55 ## 17 Amazo 85 100 85 100 100 100 ## 18 Animal Man 80 50 50 85 75 80 ## 19 Anti-Monitor 95 100 50 100 100 90 ## 20 Aquababy 60 20 15 15 40 15 ## 21 Aqualad 85 45 45 75 90 60 ## 22 Aquaman 95 85 80 80 100 80 ## 23 Ares 90 100 75 100 100 100 ## 24 Arsenal 80 55 60 60 65 85 ## 25 Atlas 85 100 45 100 30 80 ## 26 Atom 90 95 75 95 90 85 ## 27 Atom Girl 75 10 25 30 40 45 ## 28 Atom II 95 10 35 45 40 60 ## 29 Atom IV 85 55 55 55 60 65 ## 30 Azrael 85 20 20 20 35 80 ## Alignment Gender Creator TotalPower n Race_Alien ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 good Male Dark Horse Comics 445 2 0 ## 2 good Female Dark Horse Comics 340 384 0 ## 3 good Male Dark Horse Comics 285 384 0 ## 4 good Female Dark Horse Comics 375 384 0 ## 5 good Male Dark Horse Comics 410 6 0 ## 6 good Male Dark Horse Comics 330 384 0 ## 7 bad Male Dark Horse Comics 395 384 0 ## 8 good Female Dark Horse Comics 290 384 0 ## 9 good Male Dark Horse Comics 365 384 0 ## 10 bad Male Dark Horse Comics 435 8 0 ## 11 bad Male Dark Horse Comics 345 9 0 ## 12 bad Male Dark Horse Comics 445 9 0 ## 13 bad Female Dark Horse Comics 450 9 0 ## 14 good Female Dark Horse Comics 250 384 0 ## 15 good Male DC Comics 260 384 0 ## 16 good Male DC Comics 190 384 0 ## 17 bad Male DC Comics 570 8 0 ## 18 good Male DC Comics 420 384 0 ## 19 bad Male DC Comics 535 16 0 ## 20 good Male DC Comics 165 384 0 ## 21 good Male DC Comics 400 5 0 ## 22 good Male DC Comics 520 5 0 ## 23 neutral Male DC Comics 565 16 0 ## 24 good Male DC Comics 405 384 0 ## 25 bad Male DC Comics 440 16 0 ## 26 good Male DC Comics 530 384 0 ## 27 good Female DC Comics 225 384 0 ## 28 good Male DC Comics 285 384 0 ## 29 good Male DC Comics 375 384 0 ## 30 good Male DC Comics 260 384 0 ## Race_Altered Race_Amazon Race_Android Race_Animal Race_Asgardian ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 0 0 0 ## 4 0 0 0 0 0 ## 5 0 0 0 0 0 ## 6 0 0 0 0 0 ## 7 0 0 0 0 0 ## 8 0 0 0 0 0 ## 9 0 0 0 0 0 ## 10 0 0 1 0 0 ## 11 0 0 0 0 0 ## 12 0 0 0 0 0 ## 13 0 0 0 0 0 ## 14 0 0 0 0 0 ## 15 0 0 0 0 0 ## 16 0 0 0 0 0 ## 17 0 0 1 0 0 ## 18 0 0 0 0 0 ## 19 0 0 0 0 0 ## 20 0 0 0 0 0 ## 21 0 0 0 0 0 ## 22 0 0 0 0 0 ## 23 0 0 0 0 0 ## 24 0 0 0 0 0 ## 25 0 0 0 0 0 ## 26 0 0 0 0 0 ## 27 0 0 0 0 0 ## 28 0 0 0 0 0 ## 29 0 0 0 0 0 ## 30 0 0 0 0 0 ## Race_Atlantean Race_Cosmic Race_Cyborg Race_DemiHumanGod Race_Demon ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 0 0 0 ## 4 0 0 0 0 0 ## 5 0 0 0 0 1 ## 6 0 0 0 0 0 ## 7 0 0 0 0 0 ## 8 0 0 0 0 0 ## 9 0 0 0 0 0 ## 10 0 0 0 0 0 ## 11 0 0 1 0 0 ## 12 0 0 1 0 0 ## 13 0 0 1 0 0 ## 14 0 0 0 0 0 ## 15 0 0 0 0 0 ## 16 0 0 0 0 0 ## 17 0 0 0 0 0 ## 18 0 0 0 0 0 ## 19 0 0 0 0 0 ## 20 0 0 0 0 0 ## 21 1 0 0 0 0 ## 22 1 0 0 0 0 ## 23 0 0 0 0 0 ## 24 0 0 0 0 0 ## 25 0 0 0 0 0 ## 26 0 0 0 0 0 ## 27 0 0 0 0 0 ## 28 0 0 0 0 0 ## 29 0 0 0 0 0 ## 30 0 0 0 0 0 ## Race_Entity Race_Eternal Race_Frost Race_Giant Race_God Race_Human ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 1 ## 3 0 0 0 0 0 1 ## 4 0 0 0 0 0 1 ## 5 0 0 0 0 0 0 ## 6 0 0 0 0 0 1 ## 7 0 0 0 0 0 1 ## 8 0 0 0 0 0 1 ## 9 0 0 0 0 0 1 ## 10 0 0 0 0 0 0 ## 11 0 0 0 0 0 0 ## 12 0 0 0 0 0 0 ## 13 0 0 0 0 0 0 ## 14 0 0 0 0 0 1 ## 15 0 0 0 0 0 1 ## 16 0 0 0 0 0 1 ## 17 0 0 0 0 0 0 ## 18 0 0 0 0 0 1 ## 19 0 1 0 0 1 0 ## 20 0 0 0 0 0 1 ## 21 0 0 0 0 0 0 ## 22 0 0 0 0 0 0 ## 23 0 1 0 0 1 0 ## 24 0 0 0 0 0 1 ## 25 0 1 0 0 1 0 ## 26 0 0 0 0 0 1 ## 27 0 0 0 0 0 1 ## 28 0 0 0 0 0 1 ## 29 0 0 0 0 0 1 ## 30 0 0 0 0 0 1 ## Race_HumanHumanKree Race_Inhuman Race_Kryptonian Race_Metahuman ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 ## 2 0 0 0 0 ## 3 0 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## 7 0 0 0 0 ## 8 0 0 0 0 ## 9 0 0 0 0 ## 10 0 0 0 0 ## 11 0 0 0 0 ## 12 0 0 0 0 ## 13 0 0 0 0 ## 14 0 0 0 0 ## 15 0 0 0 0 ## 16 0 0 0 0 ## 17 0 0 0 0 ## 18 0 0 0 0 ## 19 0 0 0 0 ## 20 0 0 0 0 ## 21 0 0 0 0 ## 22 0 0 0 0 ## 23 0 0 0 0 ## 24 0 0 0 0 ## 25 0 0 0 0 ## 26 0 0 0 0 ## 27 0 0 0 0 ## 28 0 0 0 0 ## 29 0 0 0 0 ## 30 0 0 0 0 ## Race_Mutant Race_New Race_Radiation Race_Symbiote Race_Vampire ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 1 ## 2 0 0 0 0 0 ## 3 0 0 0 0 0 ## 4 0 0 0 0 0 ## 5 0 0 0 0 0 ## 6 0 0 0 0 0 ## 7 0 0 0 0 0 ## 8 0 0 0 0 0 ## 9 0 0 0 0 0 ## 10 0 0 0 0 0 ## 11 0 0 0 0 0 ## 12 0 0 0 0 0 ## 13 0 0 0 0 0 ## 14 0 0 0 0 0 ## 15 0 0 0 0 0 ## 16 0 0 0 0 0 ## 17 0 0 0 0 0 ## 18 0 0 0 0 0 ## 19 0 0 0 0 0 ## 20 0 0 0 0 0 ## 21 0 0 0 0 0 ## 22 0 0 0 0 0 ## 23 0 0 0 0 0 ## 24 0 0 0 0 0 ## 25 0 0 0 0 0 ## 26 0 0 0 0 0 ## 27 0 0 0 0 0 ## 28 0 0 0 0 0 ## 29 0 0 0 0 0 ## 30 0 0 0 0 0 ## # ... with 528 more rows Now, you can very easily explore each category!! Remember though, these need to be converted to factors or else they’ll be confused as integers!! magrittr package can help do this easily library(magrittr) columns &lt;- names(OOEdata)[12:length(names(OOEdata))] OOEdata &lt;- OOEdata %&lt;&gt;% mutate_at(columns, funs(factor(.))) quant_df &lt;- OOEdata %&gt;% dplyr::select(Intelligence:TotalPower, Race_Human, Race_Cyborg, Race_Mutant) ggpairs(quant_df, progress = FALSE) This figure is very messy to look at here in R Markdown - but if you eliminate a few of the variables, or use R Studio’s zoom button in the plot window, you should be able to explore this! 14.1.3 The models 14.1.3.1 CART Let’s start by building ourselves a single tree. Feel free to try this with different variables. 14.1.3.1.1 Classification example ## Let&#39;s remove &#39;neutral&#39; characters from our dataset so we can look at if we can predict ## if they are good or evil. We&#39;ll also select just a few columns to work with for ## simplicity&#39;s sake modeldat &lt;- OOEdata %&gt;% dplyr::filter(Alignment!=&#39;neutral&#39;) %&gt;% dplyr::select(Intelligence:TotalPower, Race_Animal, Race_Human, Race_Demon, Race_God, Race_Asgardian, Race_Mutant, Race_Android) form &lt;- as.formula(paste(&quot;Alignment ~ &quot;,paste(names(modeldat),collapse=&#39;+&#39;))) form &lt;- update(form, . ~ . -Alignment) ## Check out help(rpart.control) for a list of the parameters that you can change! ## Try playing with these to get an idea of how they impact a single tree ## To run a regression model, change method to &#39;anova&#39; treemod &lt;- rpart(form, method=&#39;class&#39;,data=modeldat,cp=0.001,maxdepth=6) rpart.plot(treemod,cex=0.7) 14.1.3.2 GBM.step We’re going to ‘skip’ a step here to a degree and use the gbm.step algorithm, an extension to gbm (generalized boosted regression modelling/ boosted regression trees). gbm.step will calculate the optimal tree to use by way of cross validation, rather than just building a pile of trees and letting the user decide. We’ll just use the model we had above (classification). Unfortunately, gbm.step uses column index values, rather than names! gbm.step is not particularly well oriented towards multiple class target / dependent variables gbm.step needs to use a data.frame (can’t use a tibble), and in the target MUST be a factor gbm.step ALSO needs the TARGET variable to be in the form 0/1 modeldat &lt;- modeldat %&gt;% mutate(Alignment_targ = case_when(Alignment == &#39;good&#39; ~ 0, Alignment == &#39;bad&#39; ~ 1 )) %&gt;% mutate_at(c(&#39;Alignment_targ&#39;,&#39;Gender&#39;,&#39;Creator&#39;), funs(factor(.))) modeldf &lt;- data.frame(modeldat) modeldf$Alignment_targ &lt;- as.numeric(as.character(modeldf$Alignment_targ)) gbmModel &lt;- gbm.step(data=modeldf,gbm.=c(1:6,8:17),gbm.y=18,family=&quot;bernoulli&quot;, tree.complexity = 10,learning.rate=0.001,bag.fraction=0.8,max.trees = 8000) ## ## ## GBM STEP - version 2.9 ## ## Performing cross-validation optimisation of a boosted regression tree model ## for Alignment_targ and using a family of bernoulli ## Using 531 observations and 16 predictors ## creating 10 initial models of 50 trees ## ## folds are stratified by prevalence ## total mean deviance = 1.2623 ## tolerance is fixed at 0.0013 ## ntrees resid. dev. ## 50 1.256 ## now adding trees... ## 100 1.2506 ## 150 1.2457 ## 200 1.2411 ## 250 1.2376 ## 300 1.2343 ## 350 1.2314 ## 400 1.2289 ## 450 1.2266 ## 500 1.2244 ## 550 1.2228 ## 600 1.2212 ## 650 1.22 ## 700 1.219 ## 750 1.2182 ## 800 1.2178 ## 850 1.2171 ## 900 1.2167 ## 950 1.2162 ## 1000 1.2162 ## 1050 1.216 ## 1100 1.2164 ## 1150 1.2164 ## 1200 1.2162 ## 1250 1.2165 ## 1300 1.2167 ## 1350 1.2171 ## 1400 1.2176 ## 1450 1.2183 ## 1500 1.2188 ## 1550 1.2194 ## fitting final gbm model with a fixed number of 1050 trees for Alignment_targ ## ## mean total deviance = 1.262 ## mean residual deviance = 1.048 ## ## estimated cv deviance = 1.216 ; se = 0.018 ## ## training data correlation = 0.608 ## cv correlation = 0.219 ; se = 0.047 ## ## training data AUC score = 0.857 ## cv AUC score = 0.645 ; se = 0.03 ## ## elapsed time - 0.3 minutes To get a summary of the output that shows the variable importance rankings: summary(gbmModel) ## var rel.inf ## Intelligence Intelligence 20.6827250 ## Speed Speed 11.4610143 ## Durability Durability 11.2729006 ## TotalPower TotalPower 10.8897347 ## Gender Gender 9.8297482 ## Power Power 9.6046351 ## Combat Combat 8.8328880 ## Creator Creator 6.8139959 ## Strength Strength 6.5173526 ## Race_Human Race_Human 2.8977875 ## Race_Mutant Race_Mutant 1.0265049 ## Race_God Race_God 0.1707131 ## Race_Animal Race_Animal 0.0000000 ## Race_Demon Race_Demon 0.0000000 ## Race_Asgardian Race_Asgardian 0.0000000 ## Race_Android Race_Android 0.0000000 You can also plot the partial dependence plots, and then take it further to explore interactions by plotting the perspective plots gbm.plot(gbmModel,n.plots=5,write.title=F) ## the gbm perspective plot for intelligence and speed versus the fitted values(z) gbm.perspec(gbmModel,1,3,z.range=c(0.15,0.6),theta=205) ## maximum value = 0.53 14.1.3.3 random forest Random forest is much more flexible than gbm.step in some respects - for example, you aren’t being forced to use column indices! This model can be set up much like the CART model from before. modeldat &lt;- OOEdata %&gt;% dplyr::filter(Alignment!=&#39;neutral&#39;) %&gt;% dplyr::select(Intelligence:TotalPower, Race_Animal, Race_Human, Race_Demon, Race_God, Race_Asgardian, Race_Mutant, Race_Android) ## Have to make sure these things are formatted as factors modeldat &lt;- modeldat %&gt;% mutate(Alignment_targ = case_when(Alignment == &#39;good&#39; ~ 0, Alignment == &#39;bad&#39; ~ 1 )) %&gt;% mutate_at(c(&#39;Alignment_targ&#39;,&#39;Gender&#39;,&#39;Creator&#39;), funs(factor(.))) form &lt;- as.formula(paste(&quot;Alignment_targ ~ &quot;,paste(names(modeldat),collapse=&#39;+&#39;))) form &lt;- update(form, . ~ . -Alignment) form &lt;- update(form, . ~ . -Alignment_targ) rf.model &lt;- randomForest(data=data.frame(modeldat),form,mtry=3,maxnodes=10) importance(rf.model) ## MeanDecreaseGini ## Intelligence 5.61640111 ## Strength 2.75120205 ## Speed 3.26378212 ## Durability 3.68751428 ## Power 2.00164506 ## Combat 2.69710754 ## Gender 3.65273986 ## Creator 1.95510481 ## TotalPower 3.53433695 ## Race_Animal 0.08585302 ## Race_Human 0.77356915 ## Race_Demon 0.19936608 ## Race_God 0.70108698 ## Race_Asgardian 0.13385622 ## Race_Mutant 0.57543918 ## Race_Android 0.43527047 partialPlot(rf.model,data.frame(modeldat),Intelligence) partialPlot(rf.model,data.frame(modeldat),Gender) The partial dependence plot will focus on the FIRST class. So if you’re using 0/1, then 0 will be focused on in the interpretation of the plots. This can be changed with the ‘which.class’ argument in partialPlot 14.1.4 Regression problem Here, we use gbm.step to run the regression problem. If you want to run it in random forest, give it a try! Ask your instructor for some help, or use Google! There are a lot of resources available. modeldat &lt;- OOEdata %&gt;% dplyr::filter(Alignment!=&#39;neutral&#39;) %&gt;% dplyr::select(Intelligence:TotalPower, Race_Animal, Race_Human, Race_Demon, Race_God, Race_Asgardian, Race_Mutant, Race_Android) modeldat &lt;- modeldat %&gt;% mutate_at(c(&#39;Alignment&#39;,&#39;Gender&#39;,&#39;Creator&#39;), funs(factor(.))) modeldf &lt;- data.frame(modeldat) ## use names(modeldf) to find indices gbmModel &lt;- gbm.step(data=modeldf,gbm.x=c(1,6:9,11:17),gbm.y=10,family=&quot;laplace&quot;, tree.complexity = 20,learning.rate=0.001,bag.fraction=0.5,max.trees = 8000) ## ## ## GBM STEP - version 2.9 ## ## Performing cross-validation optimisation of a boosted regression tree model ## for TotalPower and using a family of laplace ## Using 531 observations and 12 predictors ## creating 10 initial models of 50 trees ## ## folds are unstratified ## total mean deviance = 82.8383 ## tolerance is fixed at 0.0828 ## ntrees resid. dev. ## 50 82.1646 ## now adding trees... ## 100 81.0515 ## 150 80.0099 ## 200 78.9957 ## 250 78.0747 ## 300 77.2117 ## 350 76.3914 ## 400 75.6181 ## 450 74.9669 ## 500 74.3643 ## 550 73.8041 ## 600 73.2439 ## 650 72.7152 ## 700 72.2177 ## 750 71.7404 ## 800 71.2944 ## 850 70.9094 ## 900 70.5368 ## 950 70.1607 ## 1000 69.828 ## 1050 69.5046 ## 1100 69.2004 ## 1150 68.9086 ## 1200 68.6309 ## 1250 68.3832 ## 1300 68.1409 ## 1350 67.9372 ## 1400 67.7401 ## 1450 67.5465 ## 1500 67.3648 ## 1550 67.2172 ## 1600 67.0695 ## 1650 66.9401 ## 1700 66.812 ## 1750 66.6705 ## 1800 66.5596 ## 1850 66.44 ## 1900 66.3336 ## 1950 66.2491 ## 2000 66.159 ## 2050 66.0834 ## 2100 65.9997 ## 2150 65.9248 ## 2200 65.8618 ## 2250 65.7962 ## 2300 65.7352 ## 2350 65.6773 ## 2400 65.622 ## 2450 65.5787 ## 2500 65.5579 ## 2550 65.5355 ## 2600 65.5166 ## 2650 65.4954 ## 2700 65.47 ## 2750 65.4401 ## 2800 65.4134 ## 2850 65.3928 ## 2900 65.3896 ## 2950 65.3746 ## 3000 65.3472 ## 3050 65.3328 ## 3100 65.3125 ## 3150 65.3111 ## 3200 65.3022 ## 3250 65.2793 ## 3300 65.2659 ## 3350 65.2544 ## 3400 65.2394 ## 3450 65.2298 ## 3500 65.2258 ## 3550 65.2305 ## 3600 65.2263 ## 3650 65.2308 ## 3700 65.2257 ## 3750 65.2244 ## 3800 65.2387 ## 3850 65.2405 ## fitting final gbm model with a fixed number of 3750 trees for TotalPower ## ## mean total deviance = 82.838 ## mean residual deviance = 52.837 ## ## estimated cv deviance = 65.224 ; se = 1.725 ## ## training data correlation = 0.735 ## cv correlation = 0.601 ; se = 0.024 ## ## elapsed time - 0.93 minutes summary(gbmModel) ## var rel.inf ## Combat Combat 42.1908031 ## Intelligence Intelligence 23.3232063 ## Race_Human Race_Human 9.1917188 ## Creator Creator 8.9862461 ## Gender Gender 7.7090968 ## Alignment Alignment 4.2868607 ## Race_Mutant Race_Mutant 3.5157636 ## Race_God Race_God 0.7963046 ## Race_Animal Race_Animal 0.0000000 ## Race_Demon Race_Demon 0.0000000 ## Race_Asgardian Race_Asgardian 0.0000000 ## Race_Android Race_Android 0.0000000 gbm.plot(gbmModel,n.plots=5,write.title=F) ## But we are interested in gender, so let&#39;s plot that. gbm.plot(gbmModel,variable.no=4,plot.layout=c(1,1)) Interestingly, we see that there does seem to be some sort of bias towards more powerful male characters. However, bearing in mind that the predictive performance (as determined by the summary statistics) is not very high. "],
["sdm-extra-bits-and-pieces.html", "15 SDM extra bits and pieces 15.1 Spatial partial predictions", " 15 SDM extra bits and pieces This section contains some additional material that either arrived too late for beginning of the workshop or didn’t fit nicely into the existing structure. This material might be moved elsewhere at a later time. 15.1 Spatial partial predictions The SDM outputs section talked about partial dependence plots. These plots show the contribution that each predictor term makes to a model. It’s also worth knowing that these partial dependencies can be evaluated on a spatial grid, which will show the spatial pattern of the contribution that a model term makes. This can be very useful for evaluating whether a particular model term is behaving as it should. In many SDM applications we don’t tend to have predictor variables that directly tell us about processes and conditions that affect our target species. This is particularly true in the Southern Ocean because of its vast and remote nature. We typically rely on satellite- or model-derived information, which is not necessarily directly linked to the behaviour or distribution of our target species. Instead, we often rely on proxy variables. For example, we rarely have direct information about prey availability for predator species. Instead we might use e.g. satellite-derived chlorophyll, which tells us something about primary productivity, and therefore might be useful as a proxy for prey availability for higher trophic levels. But it is important to check that these proxy variables are acting in the model in a sensible way, and indeed that they are actually acting as a proxy for the process that we intended them to. 15.1.1 Example Let’s return to the example that was shown in the Overview section. This first part of the code is the same as before: library(dplyr) library(worrms) library(robis) library(blueant) library(mgcv) library(SOmap) my_cmap &lt;- if (getRversion() &gt;= &quot;3.6&quot;) { hcl.colors(21, &quot;Geyser&quot;) } else { c(&quot;#008585&quot;, &quot;#359087&quot;, &quot;#539B8A&quot;, &quot;#6DA590&quot;, &quot;#85AF97&quot;, &quot;#9BBAA0&quot;, &quot;#AEC4AA&quot;, &quot;#BED0B0&quot;, &quot;#D0DCB5&quot;, &quot;#E5E7BC&quot;, &quot;#FBF2C4&quot;, &quot;#F3E3B2&quot;, &quot;#EDD59F&quot;, &quot;#E7C68C&quot;, &quot;#E3B77A&quot;, &quot;#DEA868&quot;, &quot;#DA9857&quot;, &quot;#D58847&quot;, &quot;#D1773A&quot;, &quot;#CC6530&quot;, &quot;#C7522B&quot;) } ## taxonomy x &lt;- occurrence(datasetid = &quot;cb16377b-56a8-4d95-802d-4eec02466773&quot;) my_species &lt;- &quot;Euphausia crystallorophias&quot; tax &lt;- wm_records_names(name = my_species) my_aphia_id &lt;- tax[[1]]$valid_AphiaID ## data to presence/absence format xfit &lt;- x %&gt;% dplyr::rename(lon = &quot;decimalLongitude&quot;, lat = &quot;decimalLatitude&quot;) %&gt;% group_by(lon, lat) %&gt;% dplyr::summarize(present = any(my_aphia_id %in% aphiaID)) ## environmental data ## put the data into a temporary directory my_data_directory &lt;- tempdir() data_source &lt;- sources_sdm(&quot;Southern Ocean marine environmental data&quot;) ## fetch the data status &lt;- bb_get(data_source, local_file_root = my_data_directory, verbose = TRUE) nc_files &lt;- Filter(function(z) grepl(&quot;\\\\.nc$&quot;, z), status$files[[1]]$file) ## create a raster stack of selected layers env_stack &lt;- subset(stack(nc_files), c(&quot;depth&quot;, &quot;ice_cover_mean&quot;)) temp &lt;- as.data.frame(raster::extract(env_stack, xfit[, c(&quot;lon&quot;, &quot;lat&quot;)])) xfit &lt;- bind_cols(xfit, temp) fit &lt;- gam(present ~ s(depth) + s(ice_cover_mean) + s(lon), family = binomial, data = xfit) This gam presence/absence model is the same as in the Overview example, except that this time we have added lon (longitude) as a predictor term. You wouldn’t normally use latitude or longitude as a predictor in this way, we’re doing so here just to illustrate the example. The partial dependence plots for this model: plot(fit, pages = 1, shade = TRUE, scale = 0) We can see that the partial term for longitude (bottom-left panel) is a bit wiggly - in particular, the bump at around -60 looks a little odd. However, it’s difficult to interpret that plot to know if this is an ecologically-plausible or informative fit. Let’s make a spatial plot of that partial term. ## grid to predict onto xpred &lt;- expand.grid( lon = seq(from = floor(min(xfit$lon)), to = ceiling(max(xfit$lon)), by = 0.25), lat = seq(from = floor(min(xfit$lat)), to = ceiling(max(xfit$lat)), by = 0.25)) ## environmental data across our grid xpred &lt;- bind_cols(as.data.frame(xpred), as.data.frame(raster::extract(env_stack, xpred[, c(&quot;lon&quot;, &quot;lat&quot;)]))) ## partial prediction terms sp_pred &lt;- predict(fit, newdata = xpred, type = &quot;terms&quot;) We use predict(..., type = &quot;terms&quot;) to get the contribution of each term in the model across our xpred data (our grid). This type = &quot;terms&quot; option only works with some model types (GLMs and GAMs, for example), but analogous methods exist for other model frameworks. Now plot the spatial contribution of the lon predictor term: xpred &lt;- cbind(xpred, sp_pred) pr &lt;- rasterFromXYZ(xpred[, c(&quot;lon&quot;, &quot;lat&quot;, &quot;s(lon)&quot;)]) ## the &quot;s(lon)&quot; term is the fitted (smooth) predictor term ## plot it projection(pr) &lt;- &quot;+proj=longlat +datum=WGS84&quot; p &lt;- SOmap_auto(x = pr, bathy = pr) p$bathy_palette &lt;- my_cmap p It’s quite clear from this plot that the lon predictor term is simply allowing the model to fit higher probabilities in certain longitude bands. This doesn’t seem to be a particularly ecologically-insightful mechanism, so we’d probably drop that term from the model. We’d be better off trying to find a predictor variable that more directly explains what is happening in those bands in ecological terms. One of the dangers to watch out for is a model term that appears to fit well on the training data, but which doesn’t have a plausible interpretation. It’s possible that this term is important to the model simply because it happens by chance to explain some of the patterns in the training data. But it’s not fitting those patterns in an ecologically-meaningful way. The danger here is that inferences that are subsequently made with that model (e.g. interpreting the importance of particular processes, or using the model to make predictions in different geographic areas or under different environmental conditions) are likely to be flawed, because there is little or no underlying ecological meaning to the model term. "],
["other-r-resources-for-southern-oceanantarctic-use.html", "16 Other R resources for Southern Ocean/Antarctic use 16.1 Diet data: sohungry 16.2 Allometric equations: solong", " 16 Other R resources for Southern Ocean/Antarctic use 16.1 Diet data: sohungry The sohungry package provides access to data from the SCAR Southern Ocean Diet and Energetics Database, and some tools for working with these data. The database includes data related to diet and energy flow from conventional (e.g. gut content) and modern (e.g. molecular) studies, stable isotopes, fatty acids, and energetic content. It is a product of the SCAR community and open for all to participate in and use. See the sohungry package vignette for more information. 16.2 Allometric equations: solong The solong package provides allometric equations that relate the body size of Southern Ocean taxa to their body part measurements. It is a component of the Southern Ocean Diet and Energetics Database project. See the package reference. "]
]
